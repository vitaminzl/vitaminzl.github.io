<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GNN论文精选-第二组</title>
    <link href="/2023/08/30/gnn/notes-for-collection-2/"/>
    <url>/2023/08/30/gnn/notes-for-collection-2/</url>
    
    <content type="html"><![CDATA[<h1 id="dropedge-towards-deep-graph-convolutional-networks-on-node-classification"><strong>DropEdge</strong> Towards Deep Graph Convolutional Networks On Node Classification</h1><h2 id="drop-edge">Drop Edge</h2><p>Drop Edge<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Y. Rong, W. Huang, T. Xu, and J. Huang, “DropEdge: Towards Deep Graph Convolutional Networks on Node Classification.” arXiv, Mar. 12, 2020. doi: [10.48550/arXiv.1907.10903](https://doi.org/10.48550/arXiv.1907.10903).">[1]</span></a></sup>，其方法顾名思义，就是丢弃图中的边。</p><p>具体来说，对于一个邻接矩阵 <span class="math inline">\(A\)</span> ,对于其中的每一个元素以 <span class="math inline">\(p\)</span> 的概率置0。 这看起来和 dropout 如出一辙，事实上在实现的时候，也是采用 dropout 算子来对邻接矩阵进行处理的（dropout 算子在每一轮梯度下降时，使每一个神经元以 <span class="math inline">\(p\)</span> 的概率随机置0，使得被置0的神经元无法进行梯度更新）。</p><p>这可以说就是该方法的全部了。没错，就是这么简单。那么作者又是怎么把这么一个两句话就能讲完的方法写成一篇论文的呢？故事由此开始。</p><h2 id="overfitting-and-oversmoothing">Overfitting and Oversmoothing</h2><figure><img src="https://imagehost.vitaminz-image.top/20230806161528.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>Overfitting 是机器学习的常用概念了，具体来说就是训练集上的效果不错，但在测试集上的效果却很差。Oversmoothing 则是由Li等人<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional networks for semi-supervised learning,” in _Proceedings of the AAAI conference on artificial intelligence_, 2018.">[2]</span></a></sup>提出来的概念，而后被[[Notes for Collection 1#<strong>JKN</strong> Representation Learning on Graphs with Jumping Knowledge Networks|JKN]]、[[Notes for Collection 1#<strong>PPNP</strong> Predict then Propagate Graph Neural Networks meet Personalized PageRank|PPNP]]等模型的论文里具体说明并尝试解决的问题。具体来说，它是指GNN在层数增多后，由于特征传播与聚合的操作，所有特征表示都趋向一个稳定值，其原理类似于随机游走。这使得后期的训练变得困难，具体表现为梯度下降优化时发生梯度消失，从而使得训练损失难以下降。</p><p>而 Drop Edge 的引入可以改善这2个问题。</p><h3 id="overfitting">Overfitting</h3><p>关于过拟合，作者定性地说明了该方法是通过对数据的随机扰动而对数据进行增强改善，其类似于图像分类中对图像进行拉伸、旋转等变换。另外，这种数据扰动从期望上看与原数据相比是无偏的。如果我们以概率 <span class="math inline">\(p\)</span> 删除边，则 DropEdge 只会将邻居聚合的期望上乘上 <span class="math inline">\((1-p)\)</span>（因为采样符合伯努利分布），而这个乘数将在对其归一化后实际上消失了，因此，DropEdge不会改变邻居聚合的期望。综上，DropEdge可以看作是一个无偏数据增强技术。</p><h3 id="oversmoothing">Oversmoothing</h3><p>对于过度平滑，作者花了较大笔墨去证明。其过程有些复杂，暂时看不太懂[[[WHY]]]。</p><p>但仍然可以从实验的角度来验证这一点。实验中，作者使用该层结点表示与上一层结点表示的欧几里得距离来衡量。由下图可以知道，在训练之前（模型采用初始化权重），可以发现经过DropEdge后，相邻两层结点表示的差距始终大于未使用DropEdge的模型。图b为经过150个epoch训练后的结果，发现在5和6层，其差异为0。从训练损失来看，没有通过DropEdge根本训练不动，原因很简单，第5和第6层差异为0，从而发生了严重的梯度消失现象，无法使用梯度下降优化。</p><figure><img src="https://imagehost.vitaminz-image.top/20230806164029.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="hyperparameter">Hyperparameter</h2><p>该论文的超参数也是值得一提的。作者使用的超参数由下表列出。</p><figure><img src="https://imagehost.vitaminz-image.top/20230806164516.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>其中withloop为方法self feature modeling，来源于Fout等人的工作<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, “Protein Interface Prediction using Graph Convolutional Networks,” in _Advances in Neural Information Processing Systems_, Curran Associates, Inc., 2017. Accessed: Aug. 06, 2023. [Online]. Available:[https://proceedings.neurips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html)">[3]</span></a></sup>，其公式如下。 <span class="math display">\[ H^{(l+1)} = \sigma (\tilde AH^{(l)}W^{(l)} + H^{(l)}W^{(l)}_{self})\]</span> 表中的normalization为传播模型，它使用了如下4种模式，都是比较流行的传播模式。</p><figure><img src="https://imagehost.vitaminz-image.top/20230806164542.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="dropout-and-layer-wise-dropedge">Dropout and Layer-wise DropEdge</h2><p>Dropout 可以看作是 DropEdge 的一种特殊情形，因为当结点特征被丢弃，那么所连接的边也就没了。作者通过实验说明了 Dropout 和 DropEdge 有互补的作用，联合使用可以使得效果更好。</p><p>此外，作者还提出了 Layer-wise DropEdge。上述所使用的 DropEdge 都是针对一整个模型的，也就是说，每一次梯度下降使用一次 DropEdge 得到的 <span class="math inline">\(A_{drop}\)</span> 是针对模型的所有层。而Layer-wise 指的是每一层都独立使用一次 DropEdge，也就是说每一层的 <span class="math inline">\(A_{drop}\)</span> 是不一样的。作者通过实验说明，Layer-wise DropEdge缺失可以降低训练损失，但在验证集上的损失并没有下降。因此，作者更建议使用对面向模型的 DropEdge，而非 Layer-wise，这样可以防止过拟合，并减少不必要的计算。</p><figure><img src="https://imagehost.vitaminz-image.top/20230806181232.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="conclusion">Conclusion</h2><p>该论文借鉴了 dropout，仅仅对模型进行了一个很简单的改动，却在各个数据集上取得了非常不错的效果。也从理论上证明作者的方法可以一定程度上缓解Over Smoothing的问题。文章也教会我们如何将一个简短的方法写长，即使用大量的分析、比较，再辅之以理论证明，就可以讲好一个简单又精彩的故事。</p><h1 id="adagcn-adaboosting-graph-convolutional-networks-into-deep-models"><strong>AdaGCN</strong> Adaboosting Graph Convolutional Networks into Deep Models</h1><h2 id="adaboost">Adaboost</h2><p>论文首先是借鉴了 Adaboost 方法。</p><p>[[Ensemble Learning#Random Forest|Adaboost]] 是一种集成学习的方法。该方法可以使一组弱分类器组成一个强的集成分类。但由于其用于二分类，作者使用它的多分类形式，SAMME<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="T. Hastie, S. Rosset, J. Zhu, and H. Zou, “Multi-class AdaBoost,” _Stat. Interface_, vol. 2, no. 3, pp. 349–360, 2009, doi: [10.4310/SII.2009.v2.n3.a8](https://doi.org/10.4310/SII.2009.v2.n3.a8).">[4]</span></a></sup>。</p><figure><img src="https://imagehost.vitaminz-image.top/20230810160006.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>其区别就在于 <span class="math inline">\(\alpha\)</span> 的式子增加了 <span class="math inline">\(log(K-1)\)</span> 这一项。直观地去理解，会发现，当每一个类别的误差率大于 <span class="math inline">\(\frac{1}{K}\)</span> 时，<span class="math inline">\(\alpha\)</span> 就会大于0，这符合原来二分类的性质。</p><p>为了方便计算以及增强模型的收敛性质，作者使用它的一个改良版本 SAMME.R (R for real)。以下为它的算法描述。</p><figure><img src="https://imagehost.vitaminz-image.top/20230810150249.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>它的主要区别在于将误差率的公式，由原来的指示函数，更改为一个加权概率函数。随后的权重计算公式以及加权分类结果是通过解决以下优化方程得到的。 <span class="math display">\[\begin{align}&amp;\min _{h(x)} \operatorname{E} \bigg( \exp \bigg( -\frac{1}{K} Y^T\big(f^{(m−1)}(x)+h(x)) \bigg) |x \bigg) \\&amp;subject~to~h_1(x)+···+ h_K(x)=0\end{align}\]</span> 该方程旨在最小化集成分类器的指数损失。<strong>其中还需注意的一个是</strong>，其中的数据标签的向量<span class="math inline">\(y =(y_1,...,y_K )^T\)</span>，它并不是一个 0/1 向量，它的定义为 <span class="math display">\[yk = \left\{ \begin{align}&amp;1, &amp;&amp;if~c = k, \\&amp;−\frac{1}{K−1} , &amp;&amp;if~c \ne k.\\\end{align}\right.\]</span></p><h2 id="rnn-like">RNN-like</h2><p>该论文提出的模型架构可以概括为： <span class="math display">\[\begin{align}&amp;\hat{A}^lX = \hat A·(\hat A^{l−1} X ) \\&amp;Z^{(l)} = f _{\theta}^{(l)} (\hat A^l X ) \\&amp;Z = \operatorname{AdaBoost}(Z^{(l)}) %不能有多余的空行，不然在网页中也可能除左\end{align}\]</span> 其中 <span class="math inline">\(f_{\theta}^{(l)}\)</span> 为非线性函数，比如说2层的神经网络<span class="math inline">\(f^{(l)}_{\theta}(\hat A^l X) = \operatorname{ReLU}(\hat A^l XW ^{(0)})W^{(1)}\)</span>。但是，这里需要注意的是，该模型和Adaboost有一个重要区别，那就是对于所有的 <span class="math inline">\(l\)</span> 来说，<span class="math inline">\(f_{\theta}^{(l)}\)</span> 使用 <span class="math inline">\(\theta\)</span> 参数是基于上一层分类器 <span class="math inline">\(f_{\theta}^{(l-1)}\)</span>。而一般的Adaboost各个基分类器的参数都是独立的。</p><p>模型架构可以由如下图表示。</p><figure><img src="https://imagehost.vitaminz-image.top/20230810162254.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>可以看到，这样的结构和RNN非常相像，每一层都由来自于上一层的输入，使用的 <span class="math inline">\(\theta\)</span> 参数是共用的。也就是说，AdaGCN中每一层基分类器 <span class="math inline">\(f_{\theta}^{(l)}\)</span> 都是在上一层分类器 <span class="math inline">\(f_{\theta}^{(l-1)}\)</span> 的参数上进行更行的，这一点是和Adaboost的一个重要区别。</p><h2 id="algorithm">Algorithm</h2><p>以下为算法的伪代码。</p><figure><img src="https://imagehost.vitaminz-image.top/20230810145230.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>该流程基本在参数更新中，基本仿照了SAMME.R的算法流程。值得注意的是，该算法和我们之前所看到的那些模型有个重要的不同，那就是最后的输出不是一个模型，而是一个最终的预测结果，这一点是有些奇怪的。</p><p>因为常理来说，在训练过程之后，我们将得到一个模型，最终再使用这个模型运用到验证集、测试集上。也就是说训练和预测应当是分开的，但这个算法流程貌似是合起来的。但如果我们回想一下GCN的训练过程也许就能明白，GCN的训练是一种transductive learning，也就是说其预测的东西都是模型所见过的。所以GCN训练时，将整个数据集都输入了进去，但只使用少量标签进行训练。事实上，每次训练都可以得到整个数据集的标签，但计算损失时仅使用训练数据的标签。</p><p>那么算法也同理，训练时输入整个数据集（包括训练集、验证集、测试集），但计算损失、拟合参数时只是用训练集部分的标签。因此它的输出也自然可以是整个数据集的标签。但如果偏要将训练和预测完分离开来，则需要记录训练时每一层分类器的参数<span class="math inline">\(\theta\)</span>，还要在预测时将第3、6、7步去掉。</p><h2 id="computational-comlexity-and-netowrk-depth">Computational Comlexity and Netowrk Depth</h2><p>AdaGCN的计算复杂度，主要的有点就是可以提前计算传播矩阵，也就是对于第 <span class="math inline">\(l\)</span> 层来说，<span class="math inline">\(B^{l}=\hat A^{l}X\)</span> 可以被提前计算，这使得不需要在训练的每个epoch都再算一遍，这和SGC类似。</p><figure><img src="https://imagehost.vitaminz-image.top/20230812165605.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>经过作者的实验，也确实展现出它的高效。但是上面右边这张图就有些奇怪了，随着网络层次的加深，SGC每个epoch训练时间的增长远超AdaGCN，这不禁令人怀疑。因为对于SGC来说，同样是提前计算了传播矩阵和特征矩阵的乘积，并且其算法要比AdaGCN简单得多，怎么可能会比它慢呢？个人感觉，作者对SGC并没有采取提前计算的方法。</p><p>此外作者同样指出，AdaGCN虽然每个epoch训练时间比较短，但它所需要的epoch却更多一些。</p><figure><img src="https://imagehost.vitaminz-image.top/20230812170559.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>此外，作者还比较了各个模型加大深度后性能的变化。这里需要注意的是，模型的深度指的是特征传播的深度。事实上，AdaGCN有2个深度，一个是基分类器的数量，也就是作者所谓的深度，还有一个是 <span class="math inline">\(f_{\theta}\)</span> 这一非线性函数，事实上它可以是个深度神经网络，同样也可以有深度。作者的模型，也确实将网络深度与传播深度分离开来了，这一点和[[Notes for Collection 1#<strong>PPNP</strong> Predict then Propagate Graph Neural Networks meet Personalized PageRank|PPNP]]比较类似。</p><p>在结果上，随着深度加深，其性能确实不会有什么衰减，一定程度上减轻了Oversmoothing的问题，甚至可能略有提升。</p><p>不同于其他论文，改论文还讨论了模型的深度该如何确定，也就是基分类器的个数如何选择。其引用了[[#附录#Vapnik–Chervonenkis dimension|VC-dimesion]]的理论，大该是想说明，随着基分类器个数的增加，模型的VC维上界也会增大，从而提高模型过拟合的风险。但是总之，作者也就是说在实践中用交叉验证的方法来确定模型深度（讲了和没讲一样）。</p><h2 id="connection-with-ppnp-and-appnp.">Connection with PPNP and APPNP.</h2><p>作者认为，AdaGCN是自适应版本的APPNP。这里我没怎么看懂。我的理解时在传播过程中，APPNP里MLP参数是固定的，而AdaGCN则不是，每一层传播的参数不一样，即基分类器参数不同。因此他更具备适应性。另外有一点和APPNP是一样的，就是MLP的深度和传播深度是独立的。</p><figure><img src="https://imagehost.vitaminz-image.top/20230813211822.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>作者通过实验比较，发现在低标签率的情况下，其效果也超过APPNP（这曾是APPNP的优点之一）。</p><h2 id="conclusion-1">Conclusion</h2><p>AdaGCN借鉴集成学习中Adaboost的方法，设计了一个RNN-like的算法。其效率和性能都比较强。</p><h1 id="gcnii-simple-and-deep-graph-convolutional-networks"><strong>GCNII</strong> Simple and Deep Graph Convolutional Networks</h1><h2 id="residual-connection-and-identity-map">Residual Connection and Identity Map</h2><p>GCNII 的模型可以由如下公式表示： <span class="math display">\[H^{(l+1)} =\sigma\bigg(\bigg( (1−\alpha_l) \tilde PH^{(l)} +\alpha_l H^{(0)}\bigg)\bigg( (1−\beta_l)I_n +\beta_l W^{(l)}\bigg)\bigg)\]</span> 它由2部分组成： * 初始层残差连接：<span class="math inline">\((1−\alpha_l) \tilde PH^{(l)} +\alpha_l H^{(0)}\)</span>，注意这里的 <span class="math inline">\(H^{(0)}\)</span> 不必是 <span class="math inline">\(X\)</span>，它可以是 <span class="math inline">\(X\)</span> 经过线性变换的结果，如 <span class="math inline">\(XW\)</span>。 * 自身映射：<span class="math inline">\((1−\beta_l)I_n +\beta_l W^{(l)}\)</span></p><p>著名的 ResNet 就是利用残差连接来避免梯度消失问题，从而使得神经网络可以做得很深。但 ResNet 的残差连接，是和前几层做加法运算，这事实上在 GCN 中就已经尝试过使用这种方法了，确实在一定程度上可以缓解 Oversmoothing 的问题，但效果随着深度地加深，并没有变好。</p><p>于是在本文使用的残差是与初始层做的残差。事实上这一迭代形式和 [[Notes for Collection 1#PPNP and APPNP|APPNP]] 非常相似。这里提供了一个新的视角来理解APPNP，即和初始层做残差连接的神经网络。但和 APPNP 的区别也十分明显，APPNP 在传播过程并没有参数学习的过程，但 GCNII 每一次传播乘上参数矩阵，并使用非线性函数压缩。也就是说，在 GCNII 中，它并没有讲特征传播的深度和神经网络的深度独立开来，二者是统一的。</p><p>这里 GCNII 每一层使用的参数矩阵包含了一个自身的映射。从优化的角度来看，它有2个作用<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="M. Hardt and T. Ma, “Identity Matters in Deep Learning.” arXiv, Jul. 20, 2018. doi: [10.48550/arXiv.1611.04231](https://doi.org/10.48550/arXiv.1611.04231).">[5]</span></a></sup>：1. 使得参数矩阵 <span class="math inline">\(W\)</span> 的范数很小，这样可以使用较大的正则化系数防止过拟合；2. 优化方程的驻点就是全局最小值。</p><p>另外作者还给出了 <span class="math inline">\(\beta_l\)</span> 的选取，它的选择对于不同深度来说应当使不以言的，随着深度的加深，为了防止过拟合以及梯度消失，<span class="math inline">\(\beta_l\)</span> 应当变小，让单位矩阵占据更大比重。作者给出了调整公式： <span class="math display">\[\beta_l=log(\frac{\lambda}{l}+1)\approx\frac{\lambda}{l}\]</span> 其中 <span class="math inline">\(\lambda\)</span> 时超参数。但他貌似没有说原因，我也不知道这公式是哪来的[[[WHY]]]。</p><p>另外作者还介绍了自身映射和压缩感知（Compressive Sensing）领域中的算法 <a href="https://zhuanlan.zhihu.com/p/555651497">ISTA</a> （或者<a href="https://www.cnblogs.com/louisanu/p/12045861.html">iSTA</a>）进行了比较。ISTA 是线性问题的逆问题，通过求解以下 Lasso 正则化形式的优化方程 <span class="math display">\[\min_{x\in \mathcal{R}^n} \frac{1} {2} ‖Bx − y‖_2^2 + \lambda‖x‖_1.\]</span> 与我们常见的线性回归问题不同，这里 <span class="math inline">\(B,y\)</span> 是已知量，而 <span class="math inline">\(x\)</span> 则是待求量。它可以看作是一个信号重建的问题，经过亚采样的信号进行重建。和梯度下降类似，最终可以通过如下的迭代求解 <span class="math inline">\(x\)</span>。 <span class="math display">\[x_{t+1} = P_{\mu_tλ} (x_t − \mu_tB^T Bx_t + \mu_tB^T y) ,\]</span> <span class="math inline">\(P_β(·)\)</span> 是一个带阈值的线性函数，他的曲线如下图所示。</p><figure><img src="https://imagehost.vitaminz-image.top/20230816154534.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>如果令 <span class="math inline">\(W=-B^TB\)</span>，那么就有 <span class="math display">\[x_{t+1} = P_{\mu_tλ} \bigg((I_n + \mu W)x_t + \mu_tB^T y\bigg) ,\]</span> 于是作者认为， <span class="math inline">\((I_n + \mu W)\)</span> 就是自身映射，而 <span class="math inline">\(\mu_tB^T y\)</span> 则可以看作是初始层残差连接。另外 <span class="math inline">\(P_β(·)\)</span> 的作用可以类比为 GCNII 的非线性函数，如 <span class="math inline">\(ReLU\)</span>。作者的想法也很可能是从该算法上得到的启发。</p><p>另外，作者还提出了 GCNII 的一个变式 GCNII*，公式如下。主要区别在于，他对初始化层也乘上了一个带自身映射的参数矩阵。 <span class="math display">\[H^{(l+1)} =\sigma\bigg((1−\alpha_l) \tilde PH^{(l)} \bigg( (1−\beta_l)I_n +\beta_l W^{(l)}_1 \bigg)+\alpha_l H^{(0)}\bigg( (1−\beta_l)I_n +\beta_l W^{(l)}_2 \bigg)\bigg)\]</span></p><figure><img src="https://imagehost.vitaminz-image.top/20230816165539.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>根据最后的结果，可以看到 GCN 和 GCNII 即便在高达64层的深度，性能也并未有明显减弱，甚至会达到较高的水准。但值得一提的是，表格中没有展现 APPNP 的性能，事实上 APPNP 的性能也是有如此特点的。</p><h2 id="spectral-aanlaysis">Spectral Aanlaysis</h2><p>作者首先分析了<strong>带残差连接</strong>的 GCN，其最后也是会收敛到一个值，<span class="math inline">\(π = \frac{\langle \tilde{D}^{1/2} 1,x\rangle} {2m+n}\)</span>。</p><p>作者对每一层的结点做了分析，最后得到对于第 <span class="math inline">\(K\)</span> 层的 <span class="math inline">\(j\)</span> 结点有： <span class="math display">\[h^{(K)}(j)=\sqrt{d_j+1}\bigg(\sum_{i=1}^{n}\frac{\sqrt{d_i+1}}{2m+n}x_i\pm\frac{\sum_{i=1}^nx_i\big(1-\frac{\lambda_\tilde{G}^2}{2}\big)^K}{\sqrt{d_j+1}}\bigg)\]</span> 其中，<span class="math inline">\(\lambda_\tilde{G}\)</span> 带自环的规范化拉普拉斯矩阵的 <span class="math inline">\(\tilde L = I_n − \tilde{D}^{−1/2} \tilde{A} \tilde{D}^{−1/2}\)</span> 的最小非0特征值，<span class="math inline">\(m,n\)</span> 分别为结点和边的数量。可以看到，对于结点 <span class="math inline">\(j\)</span> 来说，如果有更大的度数 <span class="math inline">\(d_j\)</span>，那么就有更大的 <span class="math inline">\(\sqrt{d_j + 1}\)</span>，其收敛到终态的速度也就越快（注意 <span class="math inline">\(\pm\)</span> 后一项会趋向于0）。</p><p>作者也从 Spectral 的角度分析了 GCNII，作者认为，一个 K 层的 GCNII，他可以模拟以下多项式： <span class="math display">\[\sum^{K}_{l=0}\theta_l\tilde{L}^{l}\]</span> 且参数 <span class="math inline">\(\theta\)</span> 是 arbitrary 的。这个形式在 [[Notes for Collection 1#ChebNet|GCN]] 中就有提及，但需注意的是，作者认为 GCN 中的 <span class="math inline">\(\theta\)</span> 是 fixed 的[[[WHY]]]。我对这里的 fixed 和 arbitrary 不是很理解。个人感觉，大概是说对于 GCN 来说，对于某个任务他最后学得的最优参数只有一种可能，但是对于 GCNII 来说，可以用过调节 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 来获得任意可能的解。</p><figure><img src="https://imagehost.vitaminz-image.top/20230817105644.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>作者也做了实验的验证。首先结点度数对 GNN 的影响。作者根据结点的根据度数的范围 <span class="math inline">\([2^i, 2^{i+1})\)</span> ， <span class="math inline">\(i = 0, . . . , \infty\)</span>，分成了一个个组。并分别计算各个组的准确率。得到上图中的 figure 1。可以看到，对于度数会越高的组别，其准确率下降的越厉害。尤其在 Cora 和 Citeseer 度数 100 之后，甚至连 GCN-2(residual) 准确率降至0。</p><p>此外作者还做了消融实验，验证了添加的2个模块的有效性。但从图中也可以看出，虽然 GCN 做不深，但浅层的 GCN 效果也不错，而做深的 GCNII 其实并没有好太多，不得不怀疑做深是否有必要。</p><h2 id="limitation">Limitation</h2><p>文章聚焦于把网络做深，但是做深的效果并没有好太多。这种深度加大的行为，性价比可能并不高。</p><h2 id="conclusion-2">Conclusion</h2><p>作者通过加入初始化残差和自身映射，引入2个超参数调节模型，确实能把网络做的很深，不至于梯度消失、过拟合或者说Oversmoothing，其效果也略有上升，但是其性能的调高很有限。不过在大模型时代，不顾一切地把网络做深也许确实能够带来性能的极大提升，这或许也是为什么这么多论文想把 GNN 做深的原因了。</p><h1 id="deepergcn-all-you-need-to-train-deeper-gcns"><strong>DeeperGCN</strong> All You Need to Train Deeper GCNs</h1><h2 id="massage-passing">Massage Passing</h2><p>论文将 GNN 抽象成了3个阶段：1. Message Construction，2. Message Aggregation，3. Vertex Update。</p><p>这在 PyG 中，有着同样的分类。具体参考 <em><a href="https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html">Creating Message Passing Networks</a></em>。公式可描述如下： <span class="math display">\[\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}_i^{(k-1)}, \bigoplus_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right),\]</span> 其中 <span class="math inline">\(\phi,\bigoplus, \gamma\)</span> 分别表示1、2、3这3个阶段。其中，<span class="math inline">\(\bigoplus\)</span> 是一个 Permutation Invariant 的函数，而 <span class="math inline">\(\phi_k,\gamma\)</span> 往往是一个可微函数，比如MLP。</p><p>后2个阶段是常用的，但是第1个阶段，可能并未明确点出。其实，在 GCN 中，我可以将对结点特征做 Normalize 作为 Message Construction。即所有的 Message 是带权重的特征，且权重为 <span class="math inline">\(1/\sqrt{d_id_j}\)</span>。因此 <span class="math inline">\(\phi\)</span> 函数的输入需要输入结点可能需要源节点和邻居结点以及他们的边信息。</p><p>另外，在之前大部分论文中，遇到的数据集边是没有特征的，如果边存在特征，那 Message 该如何构建呢？这里提供一种论文使用的办法。</p><figure><img src="https://imagehost.vitaminz-image.top/20230819213010.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>首先，我们在对离散数据编码时，往往常用 one-hot 编码，比如在 NLP 中，在语料库中存在 N 个 Token，那就编码为 N 维的 one-hot 向量。但是 one-hot 向量有时候因为过度稀疏、维度过大，并且无法表示各个向量之间的关系，比如相近语义的词语在向量表示时应当给予更接近的距离，而非 one-hot 中，所有向量都是单位正交的。所以，有一种常用的方法，就是利用线性变换，即乘上一个参数矩阵，来对每一词语重新编码，这个过程叫做 embedding。经过这样的 embedding，每一个词语会获得新的表示想浪。如上图所示，将每一个特征值都从原来的3维向量编码成了2维向量。</p><figure><img src="https://imagehost.vitaminz-image.top/20230819213112.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>而在很多图任务中，结点特征以及边特征的每一维取值也是离散的，也就是说特征的每一维都可以进行 ont-hot 编码，比如结点的第一维特征可能取值是0和1，那么它可以编码为 <span class="math inline">\(2 \times 2\)</span> 的 one-hot 矩阵，同时也可以将其做一个 embedding 的操作。那么我们对特征的每一维都可以做相同的操作，将每一维的特征的每一个取值都编码成相同维度的向量。如上图所示，假设输入特征，第一维有2种取值，第2维有3种取值，第3维有2种取值。对每一维的特征都做一个 embedding，使得每一维的特征的每一个特征取值都对应到一个维度为2的 embedding 向量。那么最后输入的特征，根据每一维的特征取值，将对应的 embedding 向量取出相加，就是最后的特征向量了。</p><p>这样的操作，同时作用在结点特征和边特征上，使他们最后的 embedding 向量维度保持一致，这样就能更容易进行特征融合，比如把边的特征加到结点特征上，得到最终的 message。</p><p>需要注意的是，在做 embedding 的时候，也就是做了一个线性变换，但变换的参数矩阵中的参数是不知道的，有的可能会使用通过一些手段预训练的参数。但总之，一般会跟随整个模型一起训练。</p><h2 id="generalized-aggregator">Generalized Aggregator</h2><p>该文的一个核心内容就是作者提出了一个通用的信息聚合器。我们知道，常用的聚合器有mean, max, sum等。作者提出了2个可以根据参数连续变化的聚合器，并且它的变化范围是mean ~ max之间。但其中并不包含 sum，作者说 sum 容易被包括，但事实上他在附录中将其作为 Future Work。</p><p>首先是 Softmax 聚合器的公式如下： <span class="math display">\[\operatorname{SoftMax\_Agg}_{\beta}(\cdot)=\sum_{u\in \mathcal{N}(v)}\frac{\exp(\beta \mathbf{m}_{vu})}{\sum_{i\in \mathcal{N}(v)}\exp(\beta\mathbf{m}_{vi})}\cdot \mathbf{m}_{vu}\]</span> 其中，<span class="math inline">\(m_{vu}\)</span> 为 message。不过这里需要注意的是，以上是论文的形式，但在代码中发现一些区别，具体参考 <em>[[Code for Collection 2#SoftMax - Max Firstly|SoftMax - Max Firstly]]</em>。</p><p>可以发现，当 <span class="math inline">\(\beta =0\)</span> 时，它是 mean，<span class="math inline">\(\beta \rightarrow +\infty\)</span> 时，它是 max（直观的理解就是，当趋向于无穷时，最大的数和其他数的差距拉到了无穷大，所以只有最大数前的系数趋向于1，其余为0）。</p><p>此外，作者还提出了 PowerMean 聚合器： <span class="math display">\[\operatorname{Power\_Agg}(\cdot)=\bigg(\frac{1}{|\mathcal{N}(v)}\sum_{u\in\mathcal{N}{v}}\mathbf{m}^{p}_{vu}\bigg)^{\frac{1}{p}}\]</span> 这里 <span class="math inline">\(p\)</span> 是个非0值，当 <span class="math inline">\(p =1\)</span> 时，它是 mean，<span class="math inline">\(p \rightarrow +\infty\)</span> 时，它是 max，且当 <span class="math inline">\(p=-1\)</span> 时是几何均值，当 <span class="math inline">\(p\rightarrow 0\)</span> 是调和均值。具体可参考 <em><a href="https://mathworld.wolfram.com/PowerMean.html">PowerMean</a></em>。</p><p>值得注意的是，PowerMean 聚合器的特征值不能为0。实际上的实现，也和公式的表示略有区别，具体可参考 <em>[[Code for Collection 2#PowerMean - Clamp Firstly|PowerMean - Clamp Firstly]]</em>。</p><figure><img src="https://imagehost.vitaminz-image.top/20230818220732.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>另外作者认为，作者用以上的图来表示它所提出的聚合器所覆盖的范围。按照上图所示，SoftMax 和 PowerMean 仅有 Mean，Max 和 Min 的交集[[[WHY]]]。</p><p>另外，作者还尝试将这2个聚合器往 sum 上靠，然后它提出了以下的聚合器形式： <span class="math display">\[∣\mathcal{N}(v)∣^y \cdot \operatorname{Agg}(\cdot)\]</span> 他发现，当 <span class="math inline">\(y=1\)</span>，且 Agg 为 Mean 时，聚合器为 Sum。另外，在他实现的代码中，也做了进一步的调整，具体可参考 <em>[[Code for Collection 2#How to get Sum|How to get Sum]]</em>.</p><h2 id="generalized-aggregation-networks-gen">GENeralized Aggregation Networks (GEN)</h2><p>论文中，作者提出了一个新的消息传播（Message Passing）模型，或者也可将其看作是神经网络中的一个 Layer，并命名为 GEN。</p><p>第一步是 Message Construction，正如在 [[#Massage Passing|Message Passing]] 一节中所说，作者的 Message Passing，先通过了 embedding，然后 <span class="math display">\[\mathbf m^{(l)}_{vu} = \rho^{(l)}(\mathbf h^{(l)}_v , \mathbf h^{(l)}_u , \mathbf h^{(l)}_{e_{vu}} ) = \operatorname{ReLU}(\mathbf h^{(l)}_u + \mathbb{1}(\mathbf h^{(l)} _{e_{vu}} ) \cdot (\mathbf h^{(l)}_{e_{vu}} ) + \epsilon, u \in \mathcal{N} (v)\]</span> 第二步是 Message Aggregation，也就是进行消息传播，使用的聚合器可以是普通的 Mean/Sum/Max，同样可以设置为在该论文中提出的新型聚合器：Softamax、PowerMean。</p><p>第三步是 Vertex Update，作者首先对特征进行了 Normalization，再将其通过 MLP。其公式如下。其中 <span class="math inline">\(s\)</span> 是一个可学习的参数（作者也比较了采用固定参数对性能的影响，结果说明效果和聚合器有关，整体上使用可学习参数更好）。注意这里的 <span class="math inline">\(\mathbf h_v\)</span> 指的是经过<strong>第一步前</strong>的表示向量，而 <span class="math inline">\(\mathbf m_v\)</span> 则是经过<strong>第二步后</strong>的表示向量。 <span class="math display">\[\mathbf h^{(l+1)}_v = \phi^{(l)}(\mathbf h^{(l)}_v , \mathbf m^{(l)}_v ) = \operatorname{MLP}(\mathbf h^{(l)}_v + s\cdot\Vert \mathbf h^{(l)}_v\Vert_2 \cdot \frac{\mathbf m^{(l)}_v }{\Vert \mathbf m^{(l)}_v \Vert_2 })\]</span> 那么以上就是作者提出的整个 GEN Layer了。</p><p>另外作者还认为 Normalization (BatchNorm/LayerNorm) 拜访位置也会影响性能，他认为 Norm -&gt; ReLU -&gt; GEN 可以带来更好的性能。</p><figure><img src="https://imagehost.vitaminz-image.top/20230818202859.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>接着作者将 GEN Layer 和其他结构混合，提出了5个模型，如上图所示。可以看到，主要的区别在于 Normalization (BatchNorm/LayerNorm) 以及 ReLU 的位置不同、是否有残差连接、使用的聚合器是否为论文提出的SoftMax和PowerMean、聚合器的参数是固定的还是可学习的。</p><p>通过这种组合方式，旨在说明每一个变化是否能够提高性能。那么这里要说的是，哪些属于作者的模型。某种程度了上来说都属于。就算是 Plain GCN，它使用的 GEN 采用的是 Mean / Max / Sum 这类朴素的聚合器，但 GEN 中第一步的 Message Construction 和第三步的 Vertex Update 的方式都和其他模型有所不同，因此只要这 5 个模型有一个性能高即可。另外这 5 个模型并不一定是进化的关系，可以将其视作是模型的超参数。</p><h2 id="limitation-1">Limitation</h2><p>对于聚合器中 <span class="math inline">\(p\)</span> 和 <span class="math inline">\(\beta\)</span> 参数，他们的变化范围最大可以到正无穷，即无边界。这是非常不好的，因为最佳参数可能造成计算溢出。</p><h2 id="conclusion-3">Conclusion</h2><p>本文的核心主要有以下几点： 1. 提出了新的 Message Passing 的模型（或者 Layer）：GEN，它包含 1. 新的 Message Construction 的方式 2. 新的聚合器，SoftMax 和 PowerMean，他们可以在 Mean 和 Max 聚合器中连续变化 3. 新的 Vertex Update 的方式，即引入了 Message Normalization 2. 提出了新的 Normalization (BatchNorm / LayerNorm) 的摆放位置 3. 将以上 2 点和残差连接相互组合，得到5个不同的模型。</p><h1 id="grand-graph-random-neural-networks-for-semi-supervised-learning-on-graphs"><strong>GRAND</strong> Graph Random Neural Networks for Semi-Supervised Learning on Graphs</h1><h2 id="drop-nodes-and-random-propagation">Drop Nodes and Random Propagation</h2><p>第一步：和 [[#<strong>DropEdge</strong> Towards Deep Graph Convolutional Networks On Node Classification|DropEdge]] 的想法相似，该论文采用 Drop Nodes 的方法来增强原数据集。也就是说，每个结点特征 <span class="math inline">\(x\)</span> 将以概率 <span class="math inline">\(p\)</span> 随机置0，由于采样符合伯努利分布，为了使数据的期望和原数据保持一致，我们会再乘上因子 <span class="math inline">\(\frac{1}{1-p}\)</span>。这一操作事实上在 dropout 中是默认的，可参考 pytorch 文档 <em><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout">dropout</a></em>。但是要注意 Drop Nodes 是将某一结点的特征全置为0，而 Dropout 是将特征矩阵中的某一元素置为0，其细粒度不同。</p><p>第二步：DropNode 之后，作者提出了另一个操作 Random Propagation。事实上就是将 0 到 K 阶的传播矩阵求个平均值，最后和特征矩阵相乘，这个操作没什么新奇的，它可以比较大程度地保留临近结点的特征。</p><p>第三步：结束以上2个操作后，将其输入到 MLP 预测。</p><p>以上过程可以用如下公式表示： <span class="math display">\[\begin{align}M =&amp; \operatorname{drop}(\mathbf{1},p)\times \mathbf{1}^T\\% &amp;符号要放在等号后面？？\widetilde{X} =&amp; \frac{M\cdot X}{1-p} \tag{Drop Nodes}\\\overline{A}^{(s)} =&amp; \frac{1}{s+1}\sum_{i=0}^s \hat A^i\\\overline{X}^{(s)} =&amp; \overline{A}^{(s)}\widetilde{X}\tag{Random Propagation}\\\widetilde{Z}^{(s)} =&amp; \operatorname{MLP}(\overline{X},\Theta) \tag{Prediction}\end{align}\]</span></p><p>其中 Drop Nodes 可以首先获取一个 0/1 的 mask，在让 mask 和原数据相乘。 <span class="math inline">\(M\)</span> 为 Mask，<span class="math inline">\(\operatorname{drop}(\cdot,p)\)</span> 就是以 <span class="math inline">\(p\)</span> 的概率将元素随机置 0 的操作，<span class="math inline">\(\mathbf{1}\in \mathbb{R}^{N\times 1}\)</span> 是一个全 1 向量，输入的特征矩阵 <span class="math inline">\(X\in \mathbb{R}^{N\times F}\)</span>， <span class="math inline">\(\hat A\)</span> 为传播矩阵。</p><p>值得注意的是，以上公式中，假设总共传播 <span class="math inline">\(S\)</span> 次，那么可以得到 <span class="math inline">\(S\)</span> 个预测结果。</p><h2 id="sharpening-and-regularization">Sharpening and Regularization</h2><p>从前面一小节得到的 <span class="math inline">\(S\)</span> 个预测结果后，在经过以下的求均值和 sharpening 的操作。 <span class="math display">\[\begin{align}\overline{Z}&amp;=\frac{1}{S}\sum_{s=1}^{S}\widetilde Z^{(s)}\\\overline{Z}_{ij}'&amp;= \overline{Z}^{\frac{1}{T}}_{ij}\bigg/\sum_{c=0}^{C-1}\overline{Z}_{ic}^{\frac{1}{T}},(j=0,1,...,C-1)\end{align}\]</span> 求均值是好理解的，但是可能对 sharpening 不是很能理解。该操作可以让原来的概率分布更极端化，使小的更小，大的更大，尤其当 <span class="math inline">\(T\rightarrow \infty\)</span> 时，对于每个结点对应的预测将是一个 one-hot 向量。我猜测是因为上述求均值的原因，平滑化了结点预测的概率分布，不利于优化，并且得到的分类器也是不合理的。需要注意的是 <span class="math inline">\(T\)</span> 是个超参数。</p><p>那么最后在训练时有 2 个损失，1个是各个 <span class="math inline">\(\widetilde Z^{(s)}\)</span> 和真实标签的交叉熵损失，还有一个则是各个预测如 <span class="math inline">\(\widetilde{Z}^{(s_1)}\)</span> 和 <span class="math inline">\(\widetilde{Z}^{(s_2)}\)</span> 的距离，我们成为一致性 (Consistency) 损失，即我们希望各个层次的预测应当不能相差太多，要尽量的接近。后一项损失我们可以看作是一个正则化项。总的损失可表示为如下公式。 <span class="math display">\[\begin{align}\mathcal{L}_{con} &amp;= \frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{m-1} Y^T_i \log\widetilde{Z}_i^{(s)}\\\mathcal{L}_{con} &amp;= \frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{n-1}\Vert \overline{Z}_{i}'-\widetilde{Z}^{(x)}_i \Vert_2^2\\\mathcal{L} &amp;= \mathcal{L}_{sup} + \lambda\mathcal{L}_{con}\\\end{align}\]</span></p><h2 id="grand">GRAND</h2><figure><img src="https://imagehost.vitaminz-image.top/20230822115150.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>最后我们在整体看一下论文提出的模型，它可以用以上的图来表示，尤其是 DropNode 一块表现的是比较清晰的。</p><p>但更具体的，还可以用如下的伪代码来表示</p><figure><img src="https://imagehost.vitaminz-image.top/20230822115204.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="three-issues">Three Issues</h2><p>该论文从开始就提出了 GCN 广泛存在的 3 个问题：1. 过拟合；2. 鲁棒性差；3. 过度平滑。</p><p>我们来看一下该模型是否有一定程度上缓解了这些问题。</p><figure><img src="https://imagehost.vitaminz-image.top/20230822144103.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>首先是过拟合，也就是模型的泛化能力（generalization），上图中的 Figure 2 表现了不同模型训练损失和验证集损失的关系，可以看到 GRAND 模型在添加了 RP（Random Propagation） 和 CR（Consistency Regularization）模块后，训练损失和验证集损失是比较接近的。</p><figure><img src="https://imagehost.vitaminz-image.top/20230822144111.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>另外我们再来看一下鲁棒性，作者通过往数据集中随机增加伪边，以及基于元学习[[[WHAT]]]的增减边来对原数据集进行破坏，从 Figure 3 可以看出 GAT 对破坏后的数据集是比较敏感的，而GRAND展现出比较优良的性能。</p><h2 id="limitation-2">Limitation</h2><p>关于缺陷，在论文中也有提到，GRAND 是基于同配性假设的，即物以类聚。假如图缺乏同配性，即相似的结点并没有相互连接，那么效果可能会不好。这是很多半监督学习以及图神经网络共同的缺陷。</p><h2 id="conclusion-4">Conclusion</h2><p>作者的模型可以归为2个技术，一个是数据增强技术，即作者所谓的 Drop Nodes 和 Random Propagation，另一个则是正则化技术，通过让多种预测结果尽量接近，而制定正则化项。作者也确实一定程度上缓解了它提出的3个问题。</p><h1 id="附录">附录</h1><h2 id="vapnikchervonenkis-dimension">Vapnik–Chervonenkis dimension</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Y. Rong, W. Huang, T. Xu, and J. Huang, “DropEdge: Towards Deep Graph Convolutional Networks on Node Classification.” arXiv, Mar. 12, 2020. doi: <a href="https://doi.org/10.48550/arXiv.1907.10903">10.48550/arXiv.1907.10903</a>. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional networks for semi-supervised learning,” in <em>Proceedings of the AAAI conference on artificial intelligence</em>, 2018. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, “Protein Interface Prediction using Graph Convolutional Networks,” in <em>Advances in Neural Information Processing Systems</em>, Curran Associates, Inc., 2017. Accessed: Aug. 06, 2023. [Online]. Available:<a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2017/hash/f507783927f2ec2737ba40afbd17efb5-Abstract.html</a> <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>T. Hastie, S. Rosset, J. Zhu, and H. Zou, “Multi-class AdaBoost,” <em>Stat. Interface</em>, vol. 2, no. 3, pp. 349–360, 2009, doi: <a href="https://doi.org/10.4310/SII.2009.v2.n3.a8">10.4310/SII.2009.v2.n3.a8</a>. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>M. Hardt and T. Ma, “Identity Matters in Deep Learning.” arXiv, Jul. 20, 2018. doi: <a href="https://doi.org/10.48550/arXiv.1611.04231">10.48550/arXiv.1611.04231</a>. <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>GNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GNN</tag>
      
      <tag>图</tag>
      
      <tag>综述</tag>
      
      <tag>论文解读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文速读&lt;二&gt;：GNN系列</title>
    <link href="/2022/09/02/gnn/lun-wen-su-du-2-gnn-xi-lie/"/>
    <url>/2022/09/02/gnn/lun-wen-su-du-2-gnn-xi-lie/</url>
    
    <content type="html"><![CDATA[<h1 id="论文速读gnn系列">论文速读&lt;二&gt;：GNN系列</h1><p>论文速读系列的第二期，论文速读主要提取论文的Key Idea，本期为GNN系列的论文速读，选取了4篇经典论文和3篇近期的论文。<a href="https://vitaminzl.com/2022/07/16/kg/lun-wen-su-du-1-guan-xi-chou-qu/">上期</a>为知识图谱领域的关系抽取。</p><h2 id="key-idea">Key Idea</h2><h3 id="semi-supervised-classification-with-graph-convolutional-networks1"><em>Semi-supervised Classification with Graph Convolutional Networks</em><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.](https://arxiv.org/abs/1609.02907)">[1]</span></a></sup></h3><p>GCN的核心公式如下，其中<span class="math inline">\(\sigma\)</span>表示激活函数，<span class="math inline">\(\tilde A=A+I\)</span>，也就是给每个结点加个自环，<span class="math inline">\(\tilde D=\sum_i A_{ij}\)</span>。<span class="math inline">\(H^{(l)}\)</span>为第<span class="math inline">\(l\)</span>层的embedding，<span class="math inline">\(W\)</span>是参数矩阵。它的形式看起来非常简单。 <span class="math display">\[H^{(l+1)}=\sigma (\tilde D^{-\frac{1}{2}}\tilde A\tilde D^{-\frac{1}{2}}H^{(l)}W^{(l)})\]</span> 从卷积的参数共享角度来看，其实就是每个结点的将自己与附近的参数聚合。第1次聚合后每个结点获取了直接邻居结点的特征，但第2次聚合就可以获取间接邻居的特征（因为直接邻居包含了它的邻居的特征）。所以在多层聚合下，每个结点可以获得全局的信息。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-21.png"></p><h3 id="graph-attention-networks2"><em>Graph Attention Networks</em><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.](https://arxiv.org/abs/1710.10903)">[2]</span></a></sup></h3><p>GAN就是在GNN中加入注意力机制</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-3.png"></p><p>以下是注意力的计算公式，其中<span class="math inline">\(\vec h_i,shape(F,1);W,shape(F',F);\vec a,shape(2F',1)\)</span>。<span class="math inline">\(||\)</span>表示连接。看起来就是对2个向量进行线性变换后连接，再通过<span class="math inline">\(\vec a\)</span>他们获取之间的相关性，再做一个归一化。</p><p><span class="math display">\[\alpha_{ij}=\frac{\exp(LeakyReLU(\vec a^T[W\vec h_i|| W\vec h_j]))}{\sum_{k\in N_i}\exp(LeakyReLU(\vec a^T[W\vec h_i||W\vec h_j]))}\]</span> 和普通神经网络的注意力机制相同，GAN中也分为多头和单头的注意力机制。</p><p><span class="math inline">\(\vec h_i\)</span>的更新，单头注意力 <span class="math display">\[\vec h'_i = \sigma(\sum_{j\in N_i} \alpha_{ij}W\vec h^i)\]</span> 2种多头注意力的实现 <span class="math display">\[\vec h'_i = ||^{K}_{k=1} \sigma(\sum_{j\in N_i} \alpha^k_{ij}W^k\vec h^i)\\\vec h'_i = \sigma(\frac{1}{K}\sum^{K}_{k=1}\sum_{j\in N_i} \alpha^k_{ij}W^k\vec h^i)\]</span></p><h3 id="how-powerful-are-graph-neural-networks3"><em>How Powerful are Graph Neural Networks?</em><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Xu K, Hu W, Leskovec J, et al. How powerful are graph neural networks?[J]. arXiv preprint arXiv:1810.00826, 2018.](https://arxiv.org/abs/1810.00826)">[3]</span></a></sup></h3><p>这篇文章理论方面证明了GNN的上限是WL test(图同构测试)的上接，总结了信息聚合的方法以及对GNN的评价指标。 <span class="math display">\[h_{k+1}(u) = MLP((1+\epsilon) h_k(u) + \sum_{(u,v) \in E} h_k(v))\]</span> 其中<span class="math inline">\(\epsilon\)</span>是一个可学习的无理数参数。该论证证明了其可以达到WL test的上界。</p><h3 id="inductive-representation-learning-on-large-graphs4"><em>Inductive Representation Learning on Large Graphs</em><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[J]. Advances in neural information processing systems, 2017, 30.](https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html)">[4]</span></a></sup></h3><p><img src="https://imagehost.vitaminz-image.top/gnn-note-12.png"></p><p>如上图所示，GraphSAGE可以分为3个步骤：1. 对图中每个顶点邻居顶点进行采样；2. 根据聚合函数聚合邻居顶点蕴含的信息；3. 得到图中各顶点的向量表示供下游任务使用。</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-14.png"></p><p>每层的embedding生成可以用如上图的伪代码表示。其中AGGREGATE可以使用均值、池化以及LSTM函数。</p><h3 id="finding-global-homophily-in-graph-neural-networks-when-meeting-heterophily5"><em>Finding Global Homophily in Graph Neural Networks When Meeting Heterophily</em><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022.](https://arxiv.org/abs/2205.07308)">[5]</span></a></sup></h3><p>全局同质性、异质图。现有的网络通过多次的领域聚合获取远距离的信息。本文通过相关系数矩阵（类似于注意力机制）获取全局信息。且相关系数矩阵可转换为有闭式解的优化问题，经过简化，可将计算复杂度缩小至线性。</p><p>GloGNN，GloGNN++（系数矩阵加上了自相关）。</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-2.png" style="zoom:50%;"></p><p>核心公式 <span class="math display">\[H^{(0)}=(1-\alpha)MLP_1(X)+\alpha MLP_2(A)\\H^{(l+1)}=(1-\gamma)Z^{(l)*}H^{(l)}+\gamma H^{(0)}\\\]</span> <span class="math inline">\(Z^{(l)*}\)</span>通过求解优化问题的闭式解获得。</p><p>Grouping Effect：通过证明可得，<span class="math inline">\(Z^{(l*)},(Z^{(l*)})^T,H^{(l+1)}\)</span>都具有grouping effect。即当特征足够接近、结构足够接近时，对应结点的特征表示也足够接近，相关系数也足够接近。</p><h3 id="large-scale-learning-on-non-homophilous-graphs-new-benchmarks-and-strong-simple-methods6"><em>Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods</em><sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Lim D, Hohne F, Li X, et al. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods[J]. Advances in Neural Information Processing Systems, 2021, 34: 20887-20902.](https://proceedings.neurips.cc/paper/2021/hash/ae816a80e4c1c56caa2eb4e1819cbb2f-Abstract.html)">[6]</span></a></sup></h3><p><img src="https://imagehost.vitaminz-image.top/gnn-note-4.png"></p><p>LINKX是一个看起来很简单的模型，是基于MLP，并没有作卷积操作。它首先将结点的特征矩阵与邻接矩阵经过分别经过2个MLP，然后再做一个连接，接着与参数矩阵<span class="math inline">\(W\)</span>相乘，和前面经过MLP层的输出（类似于ResNet）</p><p>相加，接着经过一个激活函数和MLP，输出结果。</p><h3 id="gnnautoscale-scalable-and-expressive-graph-neural-networks-via-historical-embeddings7"><em>Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</em><sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Fey M, Lenssen J E, Weichert F, et al. Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings[C]//International Conference on Machine Learning. PMLR, 2021: 3294-3304.](http://proceedings.mlr.press/v139/fey21a.html)">[7]</span></a></sup></h3><p><img src="https://imagehost.vitaminz-image.top/gnn-note-13.png"></p><p>改论文主要聚焦于如何减小大型图数据训练的计算和内存开销。</p><p>GAS 框架有两个主要组成部分：</p><p>首先，第一部分是构建一个小批量节点（执行快速随机子采样）并修剪 GNN 计算图以仅保留小批量内的节点及其 1 跳邻居节点——这意味着 GAS 的尺度独立于 GNN 深度。其次，每当 GNN 聚合需要小批量节点嵌入时，GAS 就会从存储在 CPU 上的历史嵌入中检索它们。同时，当前小批量节点的历史嵌入也不断更新。</p><p>第二部分是与子采样的关键区别——能够使 GNN 最大限度地表达信息，并将当前的小批量数据和历史嵌入组合起来，得到完整的邻域信息并加以利用，同时确保对大型图的可扩展性。</p><p>GAS 的作者还将他们的想法整合到流行的 PyTorch 几何库中。于是可以在非常大的图上训练大多数的消息传递 GNN，同时降低 GPU 内存需求并保持接近全批次的性能（即在全图上训练时的性能）。</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1609.02907">Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</a> <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/1710.10903">Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</a> <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/1810.00826">Xu K, Hu W, Leskovec J, et al. How powerful are graph neural networks?[J]. arXiv preprint arXiv:1810.00826, 2018.</a> <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html">Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[J]. Advances in neural information processing systems, 2017, 30.</a> <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://arxiv.org/abs/2205.07308">Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022.</a> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://proceedings.neurips.cc/paper/2021/hash/ae816a80e4c1c56caa2eb4e1819cbb2f-Abstract.html">Lim D, Hohne F, Li X, et al. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods[J]. Advances in Neural Information Processing Systems, 2021, 34: 20887-20902.</a> <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><a href="http://proceedings.mlr.press/v139/fey21a.html">Fey M, Lenssen J E, Weichert F, et al. Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings[C]//International Conference on Machine Learning. PMLR, 2021: 3294-3304.</a> <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>GNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GNN</tag>
      
      <tag>图</tag>
      
      <tag>论文速读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>谱在聚类中的应用</title>
    <link href="/2022/08/30/gnn/pin-pu-zai-ju-lei-zhong-de-ying-yong/"/>
    <url>/2022/08/30/gnn/pin-pu-zai-ju-lei-zhong-de-ying-yong/</url>
    
    <content type="html"><![CDATA[<h1 id="谱在聚类中的应用">谱在聚类中的应用</h1><p>本文主要对谱聚类的几篇论文进行解读，并对一部分的结果进行复现。本文首先从谱聚类的一般过程入手，介绍传统的谱聚类方法NCuts、NJW。针对传统方法的相似度量的缺陷，引入改进方法ZP；又针对特征向量的选择问题，引入改进方法PI。结合以上2种方式，加入TKNN，引入改进方法ROSC，接着对ROSC中修正相似度矩阵的缺陷，结合trace lasso正则项，引入改进方法CAST。最后，对于ROSC和CAST中都提到的Group Effect进行解读。文章结尾补充了幂代法和矩阵求导的内容。其中我分别对PI方法和ROSC方法进行代码复现。</p><p>代码的仓库地址：<a href="https://github.com/vitaminzl/SpectralCluster">https://github.com/vitaminzl/SpectralCluster</a></p><p>备用镜像：<a href="https://gitee.com/murphy_z/spectral-cluster">https://gitee.com/murphy_z/spectral-cluster</a></p><h2 id="pipeline">Pipeline</h2><p><img src="https://imagehost.vitaminz-image.top/gnn-note-10.png"></p><p>谱聚类的一般过程如上图所示<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Kao B, Shan C, et al. CAST: a correlation-based adaptive spectral clustering algorithm on multi-scale data[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 439-449.](https://dl.acm.org/doi/abs/10.1145/3394486.3403086)">[2]</span></a></sup>。对于一系列的数据，我们首先计算数据之间的相似度矩阵<span class="math inline">\(S\)</span>，常用的相似度度量为高斯核<span class="math inline">\(S_{ij}=\exp(-\frac{||\vec x_i-\vec x_j||^2}{2\sigma^2})\)</span>，然后求其拉普拉斯矩阵<span class="math inline">\(L=D-S\)</span>，其中<span class="math inline">\(D\)</span>为对角矩阵，且<span class="math inline">\(D_{ii}=\sum A_{ij}\)</span>。然后求出拉普拉斯矩阵的特征向量，选择特征值<span class="math inline">\(k\)</span>小的特征向量进行k-means聚类。</p><p>以上过程可以理解为，将原数据利用相似度度量转化为图数据，即使得每个数据间连着一条”虚边“，相似度即为边的权重。接下来将数据转换到频域上，选择一些低频作为数据的特征向量，这是因为低频成分往往具有更高层次、更具信息量的特征，而高频成分则更可能是噪声。然后对这些特征向量进行聚类。</p><p>而这种一般的方法在多尺度的数据聚类中往往表现不佳。</p><h2 id="ncuts">NCuts</h2><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-4.png"></p><p>我们首先介绍一些远古的谱聚类方法。Normalized Cut<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Shi J, Malik J. Normalized cuts and image segmentation[J]. IEEE Transactions on pattern analysis and machine intelligence, 2000, 22(8): 888-905.](https://ieeexplore.ieee.org/abstract/document/868688)">[8]</span></a></sup>的想法来源于最小割。如果存在连通图<span class="math inline">\(\mathcal G=\{\mathcal V, \mathcal E\}\)</span>，每条边<span class="math inline">\(e_i\in \mathcal E\)</span>有着权重<span class="math inline">\(w_i\)</span>，为了使得连通图分割成2个连通子图，那么需要移掉一些边。若移掉这些边权之和最小，那么我们称这样的分割方法为最小割。这和聚类非常相似，边权可以表示点和点的联系，若存在两个类，那么类之间的联系应当是比较小的，类内的联系比较大。</p><p>所以一种可行的想法是，首先对连通图进行一次取最小割，然后再对连通子图进一步地取最小割，直到到达某一阈值为止。但显然这样存在一个问题，如上图所示，要使去掉边权之和最小，那每次分割肯可能都会倾向于指割掉一条边，这自然不合理。</p><p>一种自然的想法使对权重进行归一化处理 <span class="math display">\[NCut(A,B)=\frac{cut(A,B)}{assoc(A,V)}+\frac{cut(A,B)}{assoc(B,V)}\]</span> 其中<span class="math inline">\(assoc(A,V)\)</span>表示<span class="math inline">\(A\)</span>到所有连接的结点权重之和，<span class="math inline">\(cut(A,B)\)</span>则是分割后移掉边权之和。所以假如当某一侧的结点非常少时，那么这<span class="math inline">\(cut\)</span>的值可能比<span class="math inline">\(assoc\)</span>大，极端情况下值分割一个点，那么分母就是0了，则最后的结果是无穷大。</p><p>因此利用前面的想法，每次使用<span class="math inline">\(NCut\)</span>，然后再对子图进行<span class="math inline">\(NCut\)</span>，不断进行二分就可以了。</p><p>听起来很简单，但很遗憾的是，求最小割是一个NP难的问题，因此只能求其近似解。</p><p>通过一系列的复杂推导（太难了哈哈），可以得到<span class="math inline">\(D^{-\frac{1}{2}}LD^{-\frac{1}{2}}\)</span>第2小的特征向量就是对应的最小割。当然这里需要设定一个阈值，因为特征向量求出来是浮点数，但其数据会偏向两级。然后利用上面的二分法求解即可。</p><p>但是现在多数使用的NCuts是<span class="math inline">\(\min NCut(A,B)\)</span>问题转化为以下优化问题。 <span class="math display">\[\min \vec x^T(D^{-\frac{1}{2}}LD^{-\frac{1}{2}})\vec x\\st. \vec x^T\vec x=1\]</span> 上面这个形式其实就是瑞丽熵，那么只要取<span class="math inline">\(k\)</span>个最小的特征值对应的特征向量进行聚类即可。</p><h2 id="njw">NJW</h2><p>NJW算法<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Ng A, Jordan M, Weiss Y. On spectral clustering: Analysis and an algorithm[J]. Advances in neural information processing systems, 2001, 14.](https://proceedings.neurips.cc/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html)">[6]</span></a></sup>是取3个人名的首字母命名的。该算法和NCuts非常类似，甚至可以看作是其变形。 <span class="math display">\[\begin{align*}D^{-\frac{1}{2}}LD^{-\frac{1}{2}}&amp;=D^{-\frac{1}{2}}(D-S)D^{-\frac{1}{2}}\\&amp;=I-D^{-\frac{1}{2}}SD^{\frac{1}{2}}\end{align*}\]</span> 所以我们可以转而去求<span class="math inline">\(D^{-\frac{1}{2}}SD^{\frac{1}{2}}\)</span>最大的<span class="math inline">\(k\)</span>个特征向量，然后进行聚类。</p><h2 id="zp">ZP</h2><p>前面提到计算相似度矩阵时，我们常用高斯核<span class="math inline">\(S_{ij}=\exp(-\frac{||\vec x_i-\vec x_j||^2}{2\sigma^2})\)</span>，而高斯核中<span class="math inline">\(\sigma\)</span>的选取是需要考虑的，很多时候常常人工设定<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Zelnik-Manor L, Perona P. Self-tuning spectral clustering[J]. Advances in neural information processing systems, 2004, 17.](https://proceedings.neurips.cc/paper/2004/hash/40173ea48d9567f1f393b20c855bb40b-Abstract.html)">[3]</span></a></sup>。但更重要的是<span class="math inline">\(\sigma\)</span>是一个全局的参数，这在多尺度数据中具有一些缺陷。如设定的值较大时，稠密图的数据会趋于相似，设定较小时，稀疏图的数据则相似度过小。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-8.png"></p><p>ZP方法提出里一种局部调整<span class="math inline">\(\sigma\)</span>的设定，距离度量修正为<span class="math inline">\(S_{ij}=\exp(-\frac{||\vec x_i-\vec x_j||^2}{2\sigma_i\sigma_j})\)</span>。其中<span class="math inline">\(\sigma_i, \sigma_j\)</span>是依附于<span class="math inline">\(\vec x_i, \vec x_j\)</span>的是一种局部的参数。这一参数的设定应通过样本的特征取选取。在论文中<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Zelnik-Manor L, Perona P. Self-tuning spectral clustering[J]. Advances in neural information processing systems, 2004, 17.](https://proceedings.neurips.cc/paper/2004/hash/40173ea48d9567f1f393b20c855bb40b-Abstract.html)">[3]</span></a></sup>中选择的方法是<span class="math inline">\(\vec x_i\)</span>到第<span class="math inline">\(K\)</span>个邻居的欧式距离，实验表明<span class="math inline">\(K\)</span>取7在多个数据集上的效果表现良好。如上图所示，结点之间的边厚度表示数据间的权重大小（仅显示周围数据的权重）。图b是原来的高斯核距离度量，图c是修正后的。可以看到图b中靠近蓝点的边仍然比较厚，而图c则避免了的这种现象。</p><h2 id="pi">PI</h2><p>PI(Power Iteration)为幂迭代法<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Lin F, Cohen W W. Power iteration clustering[C]//ICML. 2010.](https://openreview.net/forum?id=SyWcksbu-H)">[5]</span></a></sup>。其灵感来源于幂迭代法用求主特征值（在<a href="##Dominant%20Eigenvalue">文章的后面部分</a>会更详细地说明）。</p><p>我们设<span class="math inline">\(W=D^{-1}A\)</span>，该矩阵有时候叫转移矩阵，因为它和马尔可夫的状态转移矩阵非常类似。每行的和为1，每个元素<span class="math inline">\(W_{i,j}\)</span>可以看作是<span class="math inline">\(i\)</span>结点到<span class="math inline">\(j\)</span>结点转移的概率。它和归一化随机游走矩阵<span class="math inline">\(L_r=I-W\)</span>有着重要的联系。NCuts算法证明了<span class="math inline">\(L_r\)</span>第2小的特征值所对应的特征向量可以作为NCuts算法的一种近似。</p><p>这里需要说明的是，<span class="math inline">\(L_r\)</span>最小的特征值为0，容易证明<span class="math inline">\(\vec 1=[1, 1, ..., 1]\)</span>是<span class="math inline">\(L_r\)</span>的0所对应的特征向量。而对于<span class="math inline">\(W\)</span>来说则，其最大的特征值为1，且<span class="math inline">\(\vec 1\)</span>是对应的特征向量。需要说明的是，<span class="math inline">\(L_r\)</span>最小的几个特征向量或<span class="math inline">\(W\)</span>最大的几个特征向量是有效的，其余可能是噪声。</p><p>首先给定一个向量<span class="math inline">\(\vec v^{(0)}= c_1\vec e_1 + c_2\vec e_2,..., +c_n\vec e_n\)</span>，其中<span class="math inline">\(\vec e_i\)</span>为<span class="math inline">\(W\)</span>的特征向量。且<span class="math inline">\(\vec e_i\)</span>所对应的特征值<span class="math inline">\(\lambda_i\)</span>满足<span class="math inline">\(1=\lambda_1 &gt; \lambda_2&gt;...&gt;\lambda_n\)</span>。</p><p><span class="math display">\[\vec v^{(t+1)} = \frac{W\vec v^{(t)}}{||W\vec v^{(t)}||_1}\]</span> 假如我们按照如上的迭代公式进行迭代，则有如下过程（暂且忽略迭代公式的分母归一化项） <span class="math display">\[\begin{align*}\vec v^{(1)} &amp;= W \vec v^{(0)} \\&amp;=c_1W\vec  e_1 + c_2W\vec e_2,..., +c_nW\vec e_n\\&amp;=c_1\lambda_1  e_1 + c_2\lambda_2\vec e_2,..., +c_n\lambda_n\vec e_n\end{align*}\]</span> 则 <span class="math display">\[\begin{align*}\vec v^{(t)} &amp;= Wv^{(t−1)} = W^2v^{(t−2)} = ... = W^tv^{(0)}\\&amp;=c_1W^t\vec  e_1 + c_2W^t\vec e_2,..., +c_nW^t\vec e_n\\&amp;=c_1\lambda_1^t  e_1 + c_2\lambda_2^t\vec e_2,..., +c_n\lambda_n^t\vec e_n\\&amp;=c_1\lambda_1^t\bigg[e_1+\sum\frac{c_2}{c_1}\bigg(\frac{\lambda_i}{\lambda_1}\bigg)^t\vec e_i)\bigg]\end{align*}\]</span> 当<span class="math inline">\(t\rightarrow +\infty\)</span>时，<span class="math inline">\(\frac{c_2}{c_1}(\frac{\lambda_i}{\lambda_1})^t\)</span>会趋向于0。该方法的提出者认为，在有效成分<span class="math inline">\(\vec e_i\)</span>的<span class="math inline">\(\lambda_i\)</span>往往会接近于<span class="math inline">\(\lambda_1\)</span>，而高频的一些噪声成分<span class="math inline">\(\lambda_j\)</span>会接近于0。在迭代过程中使得有效成分<span class="math inline">\(\vec e_i\)</span>前面的权重和噪声成分前的权重的差距会迅速扩大。</p><p>但是迭代的次数不宜过多，因为最后的结果会趋向于<span class="math inline">\(k\vec 1\)</span>，因为<span class="math inline">\(W\)</span>的主特征向量就是<span class="math inline">\(\vec 1\)</span>。因此我们需设置一个迭代的门限值，以截断迭代过程。具体的算法如下。</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-11.png" style="zoom:50%;"></p><p>这里以论文中开始提到的3圆圈数据集为例子，进行结果的复现，如下图所示。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-5.png" style="zoom:50%;"></p><p>通过以上的算法，我选取几次的迭代结果<span class="math inline">\(\vec v^{(t)}\)</span>进行可视化，同一个类别在后面的迭代过程中逐渐局部收敛到一个值。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-7.png" style="zoom: 67%;"></p><p>在实验中发现，其结果和许多因素有关，其中包括高斯核距离度量中的<span class="math inline">\(\sigma\)</span>，初始的向量<span class="math inline">\(\vec v^{(0)}\)</span>（论文中取<span class="math inline">\(\vec v^{(0)}= \frac{\sum_j A_{ij}}{\sum_i\sum_j A_{ij}}\)</span>），结束时的<span class="math inline">\(\hat\epsilon\)</span>设置，甚至发现使用<span class="math inline">\(W^T\)</span>具有更好的效果，这是因为<span class="math inline">\(W^T\)</span>的主特征值的特征向量就已经具有分类的效果（其意义尚待研究），而<span class="math inline">\(W\)</span>的主特征向量是<span class="math inline">\(\vec 1\)</span>，但这仅仅针对于3圆圈这一数据集而言。在<a href="#问题与总结">问题与总结</a>中，会提到这一点。此外还有计算机的运算精度也会影响结果。</p><p>该方法的一个重要优点是简单高效，其收敛速度快，在百万级别的数据中也能在几秒内收敛。缺陷是过分拔高了特征值大的部分，在多尺度数据中存在一些低特征值但仍然重要的信息。</p><p>以下是主函数的代码，完整代码见：https://github.com/vitaminzl/SpectralCluster/blob/master/PI.py</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    data, labels = get3CircleData(radius=[<span class="hljs-number">0.1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">17</span>], nums=[<span class="hljs-number">10</span>, <span class="hljs-number">30</span>, <span class="hljs-number">80</span>])<br>    draw3CircleData(x=data[:, <span class="hljs-number">0</span>], y=data[:, <span class="hljs-number">1</span>], labels=labels, title=<span class="hljs-string">"Data Set"</span>)<br>    S_mtx = getSimilarMatrix(data, sigma=<span class="hljs-number">1.8</span>)<br>    W = np.diag(<span class="hljs-number">1</span> / np.<span class="hljs-built_in">sum</span>(S_mtx, axis=<span class="hljs-number">0</span>)) @ S_mtx<br>    v_t = PowerIter(W, iter_nums=<span class="hljs-number">300</span>, eps=<span class="hljs-number">1e-5</span>, labels=labels)<br>    plt.show()<br></code></pre></td></tr></tbody></table></figure><h2 id="tknn">TKNN</h2><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-10.png"></p><p>聚类问题转化为图问题时，需要解决邻接问题。定义结点之间的连接常常有2种方式<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf](https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf)">[4]</span></a></sup>。第一种如上图左，每个数据选择自己最近的K个邻居相邻接，得到的图被称为K邻接图（K Nearest Neighbor Graph）；第二种如上图右，每个结点选择半径<span class="math inline">\(\epsilon\)</span>的邻居相邻接。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-14.png"></p><p>假如我们使用KNN的方法，K取4。如上图a所示，红色结点的4个最近邻用红线连接，绿色结点的4个最近邻用绿线连接。我们会发现，虽然红色的最近邻包括绿色，但绿色不包括红色，我们称红色和绿色不是<strong>相互近邻</strong>。但如图b所示，则红色和绿色则为相互近邻。若2个结点是相互近邻，则称这2个结点<strong>相互可达</strong>。如图c所示，红色与绿色是相互近邻，绿色和黄色相互近邻，那么红色和绿色也<strong>相互可达</strong>。</p><p>Transitive K Nearest Neighbor(TKNN) Graph 是指当2个结点时相互可达的，则二者连接一条边。因此其邻接矩阵<span class="math inline">\(W\)</span>中，若2个点<span class="math inline">\(i,j\)</span>相互可达，<span class="math inline">\(W_{i,j}=W_{j,i}=1\)</span>。</p><p>构造的过程可描述如下：</p><ul><li>step1: 构造K邻接矩阵<span class="math inline">\(A\)</span>，对于结点<span class="math inline">\(i\)</span>由<span class="math inline">\(k\)</span>个邻居<span class="math inline">\(j\)</span>，则<span class="math inline">\(A_{i, j}=1\)</span></li><li>step2: 构造相互近邻矩阵<span class="math inline">\(A'=A A^T\)</span>，若为相互近邻，则为1，否则为0。</li><li>step3: 寻找<span class="math inline">\(A'\)</span>的所有连通分量<span class="math inline">\(S\)</span></li><li>step4: 对于连通分量<span class="math inline">\(S_i\)</span>中的每2个元素<span class="math inline">\(S_{i,j}, S_{i,k}\)</span>，令<span class="math inline">\(W_{S_{i,j},S_{i,k}}=W_{S_{i,k},S_{i,j}}=1\)</span>，其余为0。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">getTKNN_W</span>(<span class="hljs-params">S_mtx, K</span>):<br>    N = S_mtx.shape[<span class="hljs-number">0</span>]<br>    KNN_A = np.zeros((N, N), dtype=np.int32)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        idx = np.argsort(S_mtx[i, :])<br>        KNN_A[i, idx[(N-K):]] = <span class="hljs-number">1</span><br>    MKNN_A = KNN_A * KNN_A.T<br>    G = nx.from_numpy_array(MKNN_A)<br>    compo_list = [c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> nx.connected_components(G)]<br>    TKNN_W = np.zeros((N, N), dtype=np.int32)<br>    <span class="hljs-keyword">for</span> c_i <span class="hljs-keyword">in</span> compo_list:<br>        c = np.array(<span class="hljs-built_in">list</span>(c_i), dtype=np.int32)<br>        idx_c = np.tile(c, (<span class="hljs-built_in">len</span>(c), <span class="hljs-number">1</span>))<br>        TKNN_W[idx_c.T, idx_c] = <span class="hljs-number">1</span> - np.identity(<span class="hljs-built_in">len</span>(c))<br><br>    <span class="hljs-keyword">return</span> TKNN_W<br></code></pre></td></tr></tbody></table></figure><h2 id="rosc">ROSC</h2><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-9.png"></p><p>如上图所示为ROSC方法的流程图。ROSC方法<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Kao B, Luo S, et al. Rosc: Robust spectral clustering on multi-scale data[C]//Proceedings of the 2018 World Wide Web Conference. 2018: 157-166.](https://dl.acm.org/doi/abs/10.1145/3178876.3185993)">[1]</span></a></sup>结合了以上2种方法，但相比于PI方法不同的是，它并不是将PI得到的输出直接作为k-means聚类的输入，而是增加了一个求修正相似矩阵的过程。</p><p>首先随机设置不同的<span class="math inline">\(\vec v^{(0)}\)</span>获取<span class="math inline">\(p\)</span>个“伪特征向量”，拼成一个<span class="math inline">\(p\times n\)</span>的矩阵矩阵<span class="math inline">\(X\)</span>，并对<span class="math inline">\(X\)</span>进行标准化，即使得<span class="math inline">\(XX^T=I\)</span>。</p><p>ROSC论文中认为，相似度矩阵的意义可以表示为某一个实体能够被其他实体所描述的程度。即任何一个实体<span class="math inline">\(x_{i}=\sum Z_{i,j}x_j\)</span>，这里<span class="math inline">\(Z_{i,j}\)</span>即为修正相似度矩阵。因此就有： <span class="math display">\[X=XZ+O\]</span> 其中<span class="math inline">\(O\)</span>表示噪声矩阵。</p><p>定义优化问题 <span class="math display">\[\min_{Z}||X-XZ||_F^2+\alpha_1||Z||_F+\alpha_2||W-Z||_F\]</span> 优化问题的第一项表示最小化噪声，第二项则是<span class="math inline">\(Z\)</span>的Frobenius 范数<sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://mathworld.wolfram.com/FrobeniusNorm.html](https://mathworld.wolfram.com/FrobeniusNorm.html)">[12]</span></a></sup>），为正则化项，用于平衡其他2项，第三项则是减小与前文中TKNN的邻接矩阵<span class="math inline">\(W\)</span>的差距。<span class="math inline">\(\alpha_1,\alpha_2\)</span>是平衡参数，需要人工设置。</p><p>求解以上优化问题，可以先对<span class="math inline">\(Z\)</span>求导（<a href="##Derivatives%20of%20Matrix">文章的后面</a>还会做一些补充），使导数为0即可。对三项项求导有 <span class="math display">\[\begin{align*}\frac{\partial ||X-XZ||^2_F}{\partial Z}&amp;=-2X^T(X-XZ)\\\frac{\partial\alpha_1||Z||^2_F}{\partial Z}&amp;=2\alpha_1Z\\\frac{\partial\alpha_2||W-Z||^2_F}{\partial Z}&amp;=-2\alpha_2(W-Z)\end{align*}\]</span> 三项相加有 <span class="math display">\[-X^T(X-XZ)+\alpha_1Z-\alpha_2(W-Z)=0\]</span> 整理可得 <span class="math display">\[Z^*=(2X^TX+\alpha_1 I+\alpha_2 I)^{-1}(X^TX+\alpha_2W)\]</span> 但这样求出来的<span class="math inline">\(Z^*\)</span>可能使不对称的，且可能存在负数。所以这里再次做了一个修正<span class="math inline">\(\tilde Z=(|Z^*|+|Z^*|^T)/2\)</span>。</p><p>接下来就可以执行一般的谱聚类方法了。具体的算法流程可以用如下图所示：</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-12.png" style="zoom: 33%;"></p><p>算法第4行中的whiten为白化处理<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://en.wikipedia.org/wiki/Whitening_transformation](https://en.wikipedia.org/wiki/Whitening_transformation)">[9]</span></a></sup>，是数据预处理的一种常用方法。它类似于PCA，但与PCA不同的是，PCA往往用来降维，而白化则是利用PCA的特征向量，将数据转换到新的特征空间，然后对新的坐标进行方差归一化，目的是去除输入数据的冗余信息。</p><p>根据上述算法，以下使用python对其进行复现。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ROSC</span>(<span class="hljs-params">S, C_k, t_k, alpha1, alpha2</span>):<br>    W_tknn = getTKNN_W(S, K=t_k)<br>    W = np.diag(np.<span class="hljs-built_in">sum</span>(S, axis=<span class="hljs-number">0</span>)) @ S<br>    X = prep.PIC_k(W, k=C_k)<br>    X = prep.whiten(X)<br>    X = prep.norm(X)<br>    Z = getROSC_Z(X.T, W_tknn, alpha1, alpha2)<br>    Z = (np.<span class="hljs-built_in">abs</span>(Z) + np.<span class="hljs-built_in">abs</span>(Z.T)) / <span class="hljs-number">2</span><br>    C = postp.ncuts(Z, C_k)<br>    <span class="hljs-keyword">return</span> C<br></code></pre></td></tr></tbody></table></figure><p>主函数的代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">data_name</span>):<br>    path = <span class="hljs-string">"dataset/"</span> + data_name + <span class="hljs-string">".txt"</span><br>    data = np.loadtxt(<span class="hljs-string">"dataset/Syn.txt"</span>, delimiter=<span class="hljs-string">','</span>, dtype=np.float64)<br>    label = np.loadtxt(<span class="hljs-string">"dataset/SynLabel.txt"</span>, dtype=np.int32)<br>    C_k = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(label))<br>    S = prep.getSimilarMatrix2(data=data)<br>    C = ROSC(S, C_k=C_k, t_k=t, alpha1=<span class="hljs-number">1</span>, alpha2=<span class="hljs-number">0.01</span>)<br>    prt, AMI, RI = postp.assess(label_true=label, label_pred=C)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{data_name}</span>\nPurity: <span class="hljs-subst">{prt}</span>\nAMI: <span class="hljs-subst">{AMI}</span>\nRI: <span class="hljs-subst">{RI}</span>\n"</span>)<br></code></pre></td></tr></tbody></table></figure><p>完整代码见https://github.com/vitaminzl/SpectralCluster/blob/master/ROSC.py</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-20.png"></p><p>我首先使用了人工合成的数据集，如上图左所示。使用论文中的参数效果并不是很好，然后调整了一下求解TKNN矩阵的K，原文使用的是4，我调整为8，结果效果猛增，甚至优于论文的结果，如上图右所示，只有极个别点分错。不过根据数据集调参还是不科学的哈哈哈😂。其中Purity=0.9861，AMI=0.9307，RI=0.9784。</p><p>然后我对TKNN的K参数从1到12开始调整，3个指标的变化曲线如下图所示。感觉变化还是蛮明显的。</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-17.png" style="zoom: 67%;"></p><p>以下是5个数据集的实验结果，参数除了TKNN是的K是8以外，其他都和原论文相同，即<span class="math inline">\(\alpha_1=1,\alpha_2=0.01\)</span>。大部分的数据集都没有论文的结果好（比论文结果好的加了粗），但也相差不多。除了MNist0127这个数据集是例外，其结果异常地差劲，也不知道是什么原因。</p><table><thead><tr class="header"><th>数据集</th><th>Purity</th><th>AMI</th><th>RI</th></tr></thead><tbody><tr class="odd"><td>COIL20</td><td>0.8486</td><td>0.9339</td><td>0.9683</td></tr><tr class="even"><td>Glass</td><td><strong>0.6074</strong></td><td>0.2949</td><td><strong>0.7233</strong></td></tr><tr class="odd"><td>MNIST0127</td><td>0.2767</td><td>0.0156</td><td>0.3981</td></tr><tr class="even"><td>Isolet</td><td>0.7067</td><td>0.6151</td><td>0.8459</td></tr><tr class="odd"><td>Yale</td><td>0.5636</td><td>0.3215</td><td>0.7704</td></tr></tbody></table><h2 id="trace-lasso">Trace Lasso</h2><p>Trace Lasso<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Grave E, Obozinski G R, Bach F. Trace lasso: a trace norm regularization for correlated designs[J]. Advances in Neural Information Processing Systems, 2011, 24.](https://proceedings.neurips.cc/paper/2011/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html)">[13]</span></a></sup>是一种介于L1和L2正则的正则化项。其形式为 <span class="math display">\[\Omega(W)=||XDiag(W)||_*\]</span> 其中<span class="math inline">\(X\)</span>为已归一化的特征矩阵，即对于特征<span class="math inline">\(\vec x_i\)</span>有<span class="math inline">\(\vec x_i \vec x_i^T=1\)</span>。<span class="math inline">\(W\)</span>为待求参数。<span class="math inline">\(||·||_*\)</span>为核范数<sup id="fnref:14" class="footnote-ref"><a href="#fn:14" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://en.wikipedia.org/wiki/Matrix_norm](https://en.wikipedia.org/wiki/Matrix_norm)">[14]</span></a></sup>（或迹范数）,<span class="math inline">\(||Z||_*=tr(\sqrt{Z^TZ})\)</span>，表示所有奇异值之和。</p><p>Trace Lasso具有如下性质：</p><ul><li><p>当<span class="math inline">\(X^TX=I\)</span>时，即特征之间的相关性为0，或者正交，那么 <span class="math display">\[\Omega(W)=||W||_1\]</span> 即退化为1范式。</p></li><li><p>当<span class="math inline">\(X^TX=\vec 1^T\vec 1\)</span>时，即所有特征都完全相关，那么 <span class="math display">\[\Omega(W)=||W||_2\]</span> 即退化为2范式</p></li><li><p>其他情况下，在1范式和2范式之间。</p></li></ul><p>Trace Lasso的优点就是它可以根据数据的特征，接近合适的范式，这相比弹性网络更好。</p><h2 id="cast">CAST</h2><p>ROSC方法虽然可以加强类内数据的联系，但没有使得类间的间距增大。</p><p>而CAST相比于ROSC的区别就在于修改了优化函数<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Kao B, Shan C, et al. CAST: a correlation-based adaptive spectral clustering algorithm on multi-scale data[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 439-449.](https://dl.acm.org/doi/abs/10.1145/3394486.3403086)">[2]</span></a></sup>，成为如下形式： <span class="math display">\[\min_{Z} \frac{1}{2} ||\vec x-X\vec z||_2+\alpha_1||XDiag(\vec z)||_*+\frac{\alpha_2}{2}||W-\vec z||_2\]</span> 其中<span class="math inline">\(\vec x\)</span>是<span class="math inline">\(X\)</span>的其中一个特征向量，<span class="math inline">\(z\)</span>是修正相似度矩阵<span class="math inline">\(Z\)</span>中的一个向量。于ROSC的主要区别在于范数的选择，通过trace lasso可以使得其具有类内聚合也有类外稀疏的特性。</p><p>该优化问题的求解已经超出了我的能力范围，在此直接贴出论文中的算法流程</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-15.png" style="zoom: 50%;"></p><p>整个CAST算法的流程如下：</p><p><img src="https://imagehost.vitaminz-image.top/li-spectral-cluster-16.png" style="zoom:50%;"></p><p>对比ROSC，他们的区别就在于求解<span class="math inline">\(Z^*\)</span>的方法不同，其余都是一样的。</p><p>由于Inexcat ALM的算法超出了我的知识范畴，并且最近事情较多，CAST算法的代码并没有复现。若后面有时间再填补空缺。</p><h2 id="group-effect">Group Effect</h2><p>Group Effect在ROSC和CAST论文中都提及了，以及一篇关于GNN的论文<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022](https://arxiv.org/abs/2205.07308)">[7]</span></a></sup>中也提及了，可以说精髓所在了。</p><p>首先定义一些符号：若有一系列的实体<span class="math inline">\(X=\{x_1,x_2,...,x_n\}\)</span>，设<span class="math inline">\(w_q\)</span>为<span class="math inline">\(W\)</span>的第<span class="math inline">\(q\)</span>列，设<span class="math inline">\(x_i\rightarrow x_j\)</span>表示，<span class="math inline">\(x_i^Tx_j\rightarrow 1\)</span>且<span class="math inline">\(||w_i-w_j||_2\rightarrow 0\)</span>。</p><p>如果矩阵<span class="math inline">\(Z\)</span>满足当<span class="math inline">\(x_i\rightarrow x_j\)</span>时，有<span class="math inline">\(|Z_{ip}-Z_{jp}|\rightarrow 0\)</span>，则称<span class="math inline">\(Z\)</span>具有Group Effect。</p><p>翻译成人话就是当2个实体足够接近，<span class="math inline">\(Z\)</span>矩阵中实体对应的元素也足够的接近。这说明了<span class="math inline">\(Z\)</span>矩阵确实能够反映实体之间的联系紧密程度。2个实体足够接近，它可以指实体的特征足够接近，也可以指实体附近的结构接近。事实上在另一篇关于GNN的论文<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022](https://arxiv.org/abs/2205.07308)">[7]</span></a></sup>里还包括了实体附近的结构信息。</p><p>可以证明的是，ROSC和CAST中的稀疏矩阵都有Group Effect，证明的过程过于复杂，也超出了我的能力范围了😂。</p><h2 id="补充">补充</h2><h3 id="dominant-eigenvalue">Dominant Eigenvalue</h3><p>若一个方阵<span class="math inline">\(A\)</span>存在一系列特征值<span class="math inline">\(\lambda_1,\lambda_2,...,\lambda_n\)</span>，且满足<span class="math inline">\(|\lambda_1|&gt;|\lambda_2|\ge...\ge|\lambda_n|\)</span>，则称<span class="math inline">\(\lambda_1\)</span>为该方阵的主特征值<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://www.cs.huji.ac.il/w~csip/tirgul2.pdf](https://www.cs.huji.ac.il/w~csip/tirgul2.pdf)">[10]</span></a></sup>。前文中的幂迭代法原本就是用来求主特征值的。 <span class="math display">\[\begin{align*} A^kq^{(0)} &amp;= A^{k-1}q^{(1)} = ... =Aq^{(k−1)} \\&amp;=a_1A^k\vec  e_1 + a_2A^k\vec e_2,..., +a_nA^k\vec e_n\\&amp;=a_1\lambda_1^k  e_1 + a_2\lambda_2^k\vec e_2,..., +a_n\lambda_n^k\vec e_n\\&amp;=a_1\lambda_1^k\bigg[e_1+\sum_{i=2}^{n}\frac{a_i}{a_1}\bigg(\frac{\lambda_i}{\lambda_1}\bigg)^k\vec e_i)\bigg]\end{align*}\]</span> 经过一系列迭代后又如上式子。显然，当<span class="math inline">\(k\rightarrow\infty\)</span>时，<span class="math inline">\(q^{(k)}=Aq^{(k−1)}=A^kq^{(0)}\rightarrow a_1\lambda^kx_1\)</span>。并且<span class="math inline">\([q^{k}]^TAq^{(k)}\approx [q^{(k)}]^T\lambda q^{(k)}=\lambda\|q^{(k)}\|_2=\lambda\)</span>，当<span class="math inline">\(\|q^{(k)}\|_2=1\)</span>。所以每次迭代需要对<span class="math inline">\(q\)</span>进行一次标准化。这样通过经过式子就可求出主特征值了。</p><h3 id="derivatives-of-matrix">Derivatives of Matrix</h3><p>矩阵求导是矩阵论中的相关知识<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Petersen K B, Pedersen M S. The matrix cookbook[J]. Technical University of Denmark, 2008, 7(15): 510.](https://ece.uwaterloo.ca/~ece602/MISC/matrixcookbook.pdf)">[11]</span></a></sup>，这里仅对前文用到的Frobenius范式矩阵的求导过程进行介绍。</p><p>Frobenius范式可以表示成如下的迹形式 <span class="math display">\[||X||_F=\sqrt{tr(X^TX)}\]</span> 首先引入2个求导法则</p><ul><li><p>法则1：若<span class="math inline">\(A,X\)</span>为<span class="math inline">\(m\times n\)</span>的矩阵，有 <span class="math display">\[\frac{\partial tr(A^TX)}{\partial X}=\frac{\partial tr(X^TA)}{\partial X}=A\]</span></p></li><li><p>法则2：若<span class="math inline">\(A\)</span>为<span class="math inline">\(m\times m\)</span>的矩阵，<span class="math inline">\(X\)</span>为<span class="math inline">\(m\times n\)</span>的矩阵，有</p></li></ul><p><span class="math display">\[\frac{\partial tr(X^TAX)}{\partial X}=AX+A^TX\]</span></p><p>因此若求以下导数 <span class="math display">\[\frac{\partial ||A-BX||^2_F}{\partial X}\]</span> 利用以上法则有： <span class="math display">\[\begin{align*}\frac{\partial||A-BX||^2_F}{\partial X}&amp;=\frac{\partial tr[(A-BX)^T(A-BX)]}{\partial X}\\&amp;=\frac{\partial tr[(A^T-X^TB^T)(A-BX)]}{\partial X}\\&amp;=\frac{\partial[tr(A^TA)-2tr(A^TBX)+tr(X^TB^TBX)]}{\partial X}\\&amp;=0-2A^TB+B^TBX+B^TBX\\&amp;=-2B^T(A+BX)\end{align*}\]</span></p><h2 id="问题与总结">问题与总结</h2><p>在这次深入解读论文的过程中，有很多收获，学到了很多，但也发觉不明白的东西也很多。实验中也遇到各种问题，比如在PI方法的实验中，发现对<span class="math inline">\(W^T=D^{-1}S\)</span>的最小特征向量在3圆圈数据集中，具有明显的分层，其分层特征和类别基本一致，这是一次代码写错时发现的。以及论文中出现了非常多优化问题求解，也触及到了很多知识盲区。但同时也激发了我的求知欲望，需要学的东西还有很多。</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://dl.acm.org/doi/abs/10.1145/3178876.3185993">Li X, Kao B, Luo S, et al. Rosc: Robust spectral clustering on multi-scale data[C]//Proceedings of the 2018 World Wide Web Conference. 2018: 157-166.</a> <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403086">Li X, Kao B, Shan C, et al. CAST: a correlation-based adaptive spectral clustering algorithm on multi-scale data[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 439-449.</a> <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://proceedings.neurips.cc/paper/2004/hash/40173ea48d9567f1f393b20c855bb40b-Abstract.html">Zelnik-Manor L, Perona P. Self-tuning spectral clustering[J]. Advances in neural information processing systems, 2004, 17.</a> <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf">https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a> <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://openreview.net/forum?id=SyWcksbu-H">Lin F, Cohen W W. Power iteration clustering[C]//ICML. 2010.</a> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://proceedings.neurips.cc/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html">Ng A, Jordan M, Weiss Y. On spectral clustering: Analysis and an algorithm[J]. Advances in neural information processing systems, 2001, 14.</a> <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><a href="https://arxiv.org/abs/2205.07308">Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022</a> <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span><a href="https://ieeexplore.ieee.org/abstract/document/868688">Shi J, Malik J. Normalized cuts and image segmentation[J]. IEEE Transactions on pattern analysis and machine intelligence, 2000, 22(8): 888-905.</a> <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span><a href="https://en.wikipedia.org/wiki/Whitening_transformation">https://en.wikipedia.org/wiki/Whitening_transformation</a> <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span><a href="https://www.cs.huji.ac.il/w~csip/tirgul2.pdf">https://www.cs.huji.ac.il/w~csip/tirgul2.pdf</a> <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span><a href="https://ece.uwaterloo.ca/~ece602/MISC/matrixcookbook.pdf">Petersen K B, Pedersen M S. The matrix cookbook[J]. Technical University of Denmark, 2008, 7(15): 510.</a> <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span><a href="https://mathworld.wolfram.com/FrobeniusNorm.html">https://mathworld.wolfram.com/FrobeniusNorm.html</a> <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span><a href="https://proceedings.neurips.cc/paper/2011/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html">Grave E, Obozinski G R, Bach F. Trace lasso: a trace norm regularization for correlated designs[J]. Advances in Neural Information Processing Systems, 2011, 24.</a> <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:14" class="footnote-text"><span><a href="https://en.wikipedia.org/wiki/Matrix_norm">https://en.wikipedia.org/wiki/Matrix_norm</a> <a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>谱图理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>频域</tag>
      
      <tag>python</tag>
      
      <tag>聚类</tag>
      
      <tag>谱聚类</tag>
      
      <tag>多尺度数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图（Graph）数据的频域</title>
    <link href="/2022/08/20/gnn/tu-graph-shu-ju-de-pin-yu/"/>
    <url>/2022/08/20/gnn/tu-graph-shu-ju-de-pin-yu/</url>
    
    <content type="html"><![CDATA[<h1 id="图graph数据的频域">图（Graph）数据的频域</h1><p>图神经网络（GNN）是近年来愈发火热，用以对图（Graph）数据进行特征提取以及各种下游任务。注意这里的图（Graph）应和图像（Image）区分，图是一种由点集与边集组成的数据结构，常记作<span class="math inline">\(\mathcal{G}=(\mathcal{V,E})\)</span>。以下是我学习谱图理论时的一些记录。本文先先从基变换的角度入手，说明了标准正交基的性质，并从实数域拓展到了复数域，接着从基变换的角度阐述了傅里叶变换的原理，然后从特征值与特征向量入手，通过分析拉普拉斯算子的特征向量，理解其与傅里叶变换的等价性。最后使用图的数据结构表示图像，可视化其特征向量，直观感受低频到高频分量的差异。本人才疏学浅，错误难免，欢迎交流指正。</p><h2 id="内积与基">内积与基</h2><p>我们首先回顾一下线性代数的知识。本科第一次线性代数的时候，都是以繁杂的计算与证明为主，未曾有更直观、直觉的方式去理解。这里我想从线性变换的角度，讲述线性代数中与本文内容相关的知识。</p><p>我们定义实数向量<span class="math inline">\(\vec x=[x_1, x_2,...,x_n]^T\)</span>与实数向量的<span class="math inline">\(\vec x=[x_1, x_2,...,x_n]^T\)</span>内积为<span class="math inline">\(&lt;\vec x,\vec y&gt;=\sum x_iy_i\)</span>。这是高中就学过的知识。但这里，我需要对向量的内积做一些扩充，即复数向量内积的定义。根据hermitian内积的定义<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://mathworld.wolfram.com/HermitianInnerProduct.html](https://mathworld.wolfram.com/HermitianInnerProduct.html)">[1]</span></a></sup>，复平面内积定义为<span class="math inline">\(&lt;\vec x,\vec y&gt;=\sum x_i\bar y_i\)</span>。</p><p>如果我们将向量内积扩展到函数空间，我们可以将函数视为无限长的向量，如<span class="math inline">\(f(x)=[f(x_1),f(x_2),...]\)</span>，可以定义函数的内积<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://mathworld.wolfram.com/HilbertSpace.html](https://mathworld.wolfram.com/HilbertSpace.html)">[2]</span></a></sup><span class="math inline">\(&lt;f,g&gt;=\int_a^b f(x)g(x)dx\)</span>。若函数在复平面上，则可定义为<span class="math inline">\(&lt;f,g&gt;=\int_a^b f(x)\overline{g(x)}dx\)</span>（有时共轭会放在左边）。</p><p>然后我们来回顾一下向量空间中的基。由一组线性无关的向量可以张成一个向量空间，空间中的任意向量都可以使用这一组向量的线性组合表示。这组向量称作基向量。如果基向量构成的矩阵<span class="math inline">\(A=[\vec\alpha_1,\vec\alpha_2,...,\vec\alpha_n]^T\)</span>满足<span class="math inline">\(AA^T=E\)</span>，则这组基向量称为正交基，若还满足<span class="math inline">\(||\vec\alpha_i||=1,(i=1,2,...,n)\)</span>，则称为标准正交基。如，二维向量<span class="math inline">\([1,0]^T,[0,1]^T\)</span>构成一组标准正交基。在复平面中<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://math.mit.edu/~gs/linearalgebra/](https://math.mit.edu/~gs/linearalgebra/)">[6]</span></a></sup>，转置被描述为<span class="math inline">\((A^H)_{ij}=\overline{A_{ji}}\)</span>，相比实数域，处了转置还要做一次共轭，称为共轭转置。那么正交基构成的矩阵<span class="math inline">\(A\)</span>应满足<span class="math inline">\(AA^H=E\)</span>。</p><p>假如有标准正交基<span class="math inline">\(\vec e_1, \vec e_2,...,\vec e_n\)</span>，若该空间中向量 <span class="math display">\[\vec v=w_1\vec e_1+w_2\vec e_2+,...+w_n\vec e_n=[\vec e_1,\vec e_2,..., \vec e_n][w_1,w_2,...,w_m]^T\]</span> 则 <span class="math display">\[[w_1,w_2,...,w_m]^T=[\vec e_1,\vec e_2,... ,\vec e_n]^{-1}\vec v=[\vec e_1,\vec e_2,... ,\vec e_n]^{T}\vec v\]</span> 可以看到，若想获得一组某向量在一组标准正交基上的表示，只需要让该向量与这组正交基做内积即可。</p><p>函数也存在着基的概念，若函数<span class="math inline">\(f(x)=a_ng(x)+b_nh(x)\)</span>。那么<span class="math inline">\(g(x)\)</span>和<span class="math inline">\(h(x)\)</span>就是<span class="math inline">\(f(x)\)</span>的基。若内积<span class="math inline">\(&lt;g,h&gt;=0\)</span>，<span class="math inline">\(g\)</span>和<span class="math inline">\(h\)</span>是一组正交基。</p><h2 id="傅里叶变换">傅里叶变换</h2><p><img src="https://imagehost.vitaminz-image.top/gnn-note-5.png"></p><p>在学习傅里叶变换时，想必类似上面的图大家已经见过很多次了，这里我们暂且不从几何的角度去解释，从基的角度去阐述。大多数教材都会先从傅里叶级数入手，周期函数可以表示为许多正弦信号的叠加，有如下形式 <span class="math display">\[f(t)=\frac{a_0}{2}+\sum_{n=1}^{+\infty} [a_n\sin(\frac{n\pi}{l} t )+b_n\cos({\frac{n\pi}{l} t})]\]</span> 其中，<span class="math inline">\(n\)</span>是整数，<span class="math inline">\(l\)</span>为半周期。<span class="math inline">\(1,\sin(n\omega t ),\cos({n\omega t})\)</span>可以看作是<span class="math inline">\(f(t)\)</span>许许多多相互正交的基。这是因为<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="同济大学数学系．高等数学 [M]，第七版，高等教育出版社，2014-07-04">[3]</span></a></sup>：<span class="math inline">\(\int^{\pi}_{-\pi}\cos nxdx=0\)</span>,<span class="math inline">\(\int^{\pi}_{-\pi}\sin nxdx=0\)</span>, <span class="math inline">\(\int^{\pi}_{-\pi}\cos n_1x\sin n_2xdx=0\)</span>, <span class="math inline">\(\int^{\pi}_{-\pi}\cos n_1x\cos n_2xdx=0\)</span>,<span class="math inline">\(\int^{\pi}_{-\pi}\sin n_1x\sin n_2xdx=0\)</span>。</p><p>如果使用Euler公式替换， 可以转换为如下形式 <span class="math display">\[f(t)=\sum^{+\infty}_{n=-\infty}c_ne^{i\omega t}\]</span> 其中，<span class="math inline">\(c_n=\frac{1}{l}\int_{-l}^{l}f(t)e^{-i\omega t}dt\)</span>。<span class="math inline">\(l\)</span>为半周期，<span class="math inline">\(\omega=\frac{n\pi}{l}\)</span>，即频率（有些地方会将<span class="math inline">\(\omega\)</span>视作角速度，指数项写作<span class="math inline">\(e^{2i\pi\omega t}\)</span>）。这里<span class="math inline">\(e^{i\omega t}\)</span>也是许多的标准正交基，这是因为<span class="math inline">\(\int e^{i\omega_1 t}e^{-i\omega_2 t}=0\)</span>，<span class="math inline">\(||e^{i\omega}||=1\)</span>。那么当<span class="math inline">\(l\rightarrow +\infty\)</span>时，<span class="math inline">\(c_n\)</span>就是傅里叶变换的形式了。即 <span class="math display">\[\mathcal{F}[f(t)]=\hat f(\omega)=\int_{-\infty}^{+\infty}f(t)e^{-i\omega t}dt\]</span> 所以像函数<span class="math inline">\(\hat f(\omega)\)</span>其实就是<span class="math inline">\(\omega\)</span>对应的傅里叶基<span class="math inline">\(e^{i\omega t}\)</span>的系数。上述式子也可以看作是<span class="math inline">\(f(t)\)</span>与正交基<span class="math inline">\(e^{i\omega t}\)</span>的内积，在上一节中提到了，若某个向量希望使用某组正交基来表示，拿就让这个向量和这组标准正交基做内积，标准正交基就像一个筛子，把基方向的分量提取出来了。</p><p>在计算机中，使用离散傅里叶变换（DFT）的算法计算：被描述为<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://en.wikipedia.org/wiki/Discrete_Fourier_transform](https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis)">[8]</span></a></sup> <span class="math display">\[X_k=\sum_{n=0}^{N-1}x_n \cdot e^{-\frac{i2\pi}{N}kn}\]</span> 实际计算时如快速傅里叶变换（FFT），使用如下DFT矩阵<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://en.wikipedia.org/wiki/DFT_matrix](https://en.wikipedia.org/wiki/DFT_matrix)">[9]</span></a></sup>与向量<span class="math inline">\(\vec x=[x_1,x_2,...,x_n]^T\)</span>相乘。 <span class="math display">\[W = \frac{1}{\sqrt{N}} \begin{bmatrix}1&amp;1&amp;1&amp;1&amp;\cdots &amp;1 \\1&amp;\omega&amp;\omega^2&amp;\omega^3&amp;\cdots&amp;\omega^{N-1} \\1&amp;\omega^2&amp;\omega^4&amp;\omega^6&amp;\cdots&amp;\omega^{2(N-1)}\\ 1&amp;\omega^3&amp;\omega^6&amp;\omega^9&amp;\cdots&amp;\omega^{3(N-1)}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\1&amp;\omega^{N-1}&amp;\omega^{2(N-1)}&amp;\omega^{3(N-1)}&amp;\cdots&amp;\omega^{(N-1)(N-1)}\end{bmatrix}\]</span> 其中<span class="math inline">\(\omega = e^{-2\pi i/N}\)</span>。易证明<span class="math inline">\(WW^H=E\)</span>，即它由一组标准正交基构成。具体的计算方法使用快速傅里叶变换，可以将复杂度降至<span class="math inline">\(O(n\log n)\)</span>，具体过程可以参考<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://www.bilibili.com/video/BV1za411F76U?spm_id_from=333.337.search-card.all.click&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45](https://www.bilibili.com/video/BV1za411F76U?spm_id_from=333.337.search-card.all.click&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45)">[4]</span></a></sup>。</p><h2 id="特征值与特征向量">特征值与特征向量</h2><p>简单地回顾一些特征值与特征向量的定义，设有矩阵<span class="math inline">\(A\)</span>，若存在<span class="math inline">\(\lambda,\vec x\)</span>，使得<span class="math inline">\(A\vec x=\lambda \vec x\)</span>，则<span class="math inline">\(\lambda\)</span>称为<span class="math inline">\(A\)</span>的特征值，<span class="math inline">\(\vec x\)</span>则称为特征向量，<span class="math inline">\(\lambda\)</span>的值可以不止一个，同一个<span class="math inline">\(\lambda\)</span>可以对应多个线性无关的<span class="math inline">\(\vec x\)</span>。如果想了解特征值与特征向量的几何解释，可以参考<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://www.bilibili.com/video/BV1ys411472E?p=14&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45](https://www.bilibili.com/video/BV1ys411472E?p=14&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45)">[5]</span></a></sup>，这不是本文的重点。</p><p>接下啦我想说明的是一种特殊的矩阵：实对称矩阵，即由实数组成的对称矩阵（若<span class="math inline">\(A=A^T\)</span>则称<span class="math inline">\(A\)</span>为对称矩阵）。该矩阵有很多优良的性质。首先，<span class="math inline">\(N\)</span>阶是对称矩阵，具有<span class="math inline">\(N\)</span>个特征值以及<span class="math inline">\(N\)</span>个正交的特征向量。且实对称矩阵一定可以相似对角化，即<span class="math inline">\(Q^{-1}AQ=Q^TAQ=\Lambda\)</span>。对角矩阵<span class="math inline">\(\Lambda\)</span>对角线上为特征值<span class="math inline">\(\lambda_1,\lambda_2,...\)</span>，分别对应<span class="math inline">\(Q=[\vec q_1, \vec q_2,...]\)</span>中的特征向量<span class="math inline">\(\vec q_1,\vec q_2,...\)</span>。</p><p>由于其<span class="math inline">\(Q\)</span>由一组线性无关的正交特征向量组成，他们可以构成一组正交基，特征向量组成的基也成为特征基。我们举个相似对角化的简单应用。假如要求实对称矩阵<span class="math inline">\(A\)</span>的幂次预算，如<span class="math inline">\(A^{100}\)</span>。直接计算是很麻烦的，若首先将其转化为对角矩阵<span class="math inline">\(Q^{-1}AQ=\Lambda\)</span>，那么<span class="math inline">\(Q^{-1}AQ...Q^{-1}AQQ^{-1}AQ=Q^{-1}A^{100}Q=\Lambda^{100}\)</span>。则<span class="math inline">\(A^{100}=Q\Lambda^{100}Q^{-1}\)</span>。对角矩阵的100次幂是非常容易计算的，这就使得计算量大大减小。</p><p>特征值与特征向量在微分方程中也有着重要应用<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://math.mit.edu/~gs/linearalgebra/](https://math.mit.edu/~gs/linearalgebra/)">[6]</span></a></sup>。如对一个一阶微分方程<span class="math inline">\(\frac{du}{dt}=\lambda u\)</span>。它的特解为<span class="math inline">\(e^{\lambda t}\)</span>，通解为<span class="math inline">\(u(t)=Ce^t\)</span>。假如<span class="math inline">\(A\)</span>是一个常数矩阵，<span class="math inline">\(\vec u=[u_1,u_2,...,u_n]\)</span>。<span class="math inline">\(\frac{d\vec u}{dt}=A \vec u\)</span>，那么这就是在求解一个微分线性方程组。我们可以验证的是（将以下2式子带入原方程就可以证得），当<span class="math inline">\(A\vec x= \lambda \vec x\)</span>时，<span class="math inline">\(\vec u\)</span>的一个特解为<span class="math inline">\(\vec u = e^{\lambda t}\vec x\)</span>，<span class="math inline">\(\vec x\)</span>是一个常数向量。那么解原方程组转化为求<span class="math inline">\(A\vec x= \lambda \vec x\)</span>的问题求出特征值与特征向量，最后的通解即为<span class="math inline">\(C_1e^{\lambda_1 t}\vec x_1+C_2e^{\lambda_2 t}\vec x_2,....\)</span>。</p><p>若我们将一阶微分记作<span class="math inline">\(\nabla\)</span>，则<span class="math inline">\(\frac{d\vec u}{dt} = \nabla \vec u\)</span>，带入一个特解可以得到<span class="math inline">\(\nabla \vec u=\nabla e^{\lambda t}\vec x = \lambda e^{\lambda t}\vec x\)</span>。我们可以将<span class="math inline">\(\lambda\)</span>视为<span class="math inline">\(\Delta\)</span>的特征值，<span class="math inline">\(e^{\lambda t}\)</span>为特征向量。假如是二阶微分方程<span class="math inline">\(\frac{d^2\vec u}{dt^2}=A \vec u\)</span>，会有所不同，若满足<span class="math inline">\(A\vec x= \lambda \vec x\)</span>，其特解为<span class="math inline">\(\lambda e^{i\omega t} \vec x\)</span>，其中<span class="math inline">\(\omega^2=-\lambda\)</span>。同上，若我们将二阶微分算子记为<span class="math inline">\(\Delta\)</span>，则有<span class="math inline">\(\Delta \vec u =\Delta e^{i\omega t}\vec x= \lambda e^{i\omega t}\vec x\)</span>。那么<span class="math inline">\(e^{i\omega t}\vec x\)</span>就是<span class="math inline">\(\Delta\)</span>的特征向量。</p><h2 id="拉普拉斯矩阵">拉普拉斯矩阵</h2><p><img src="https://imagehost.vitaminz-image.top/gnn-note-7.png"></p><p>对于如上的图数据，它的拉普拉斯矩阵如下。其计算方法是<span class="math inline">\(D-A\)</span>。<span class="math inline">\(D\)</span>是每个结点的度组成的对角矩阵，<span class="math inline">\(A\)</span>是图的邻接矩阵。 <span class="math display">\[L=\left(\begin{array}{rrrrrr}   2 &amp; -1 &amp;  0 &amp;  0 &amp; -1 &amp;  0\\  -1 &amp;  3 &amp; -1 &amp;  0 &amp; -1 &amp;  0\\   0 &amp; -1 &amp;  2 &amp; -1 &amp;  0 &amp;  0\\   0 &amp;  0 &amp; -1 &amp;  3 &amp; -1 &amp; -1\\  -1 &amp; -1 &amp;  0 &amp; -1 &amp;  3 &amp;  0\\   0 &amp;  0 &amp;  0 &amp; -1 &amp;  0 &amp;  1\\\end{array}\right)\]</span> 拉普拉斯矩阵是拉普拉斯算子在图数据中的表示方式<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf](https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf)">[10]</span></a></sup>，也被称为离散拉普拉斯算子。拉普拉斯算子是一个二阶微分算子，记作<span class="math inline">\(\Delta f = \sum_{i=1}^n \frac {\partial^2 f}{\partial x^2_i}\)</span>，在上一节的结尾，我们其实提到了这个算子。它的特征向量其实就是傅里叶变换中的正交基。实际上，实际上函数的傅里叶变换是定义在所有欧几里得空间上的函数的分解到其在拉普拉斯算子连续谱中的分量<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis](https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis)">[7]</span></a></sup>。</p><p>拉普拉斯矩阵是一个实对称矩阵，上一节中提到实对称矩阵的一些优良性质，存在标准正交基构成的矩阵<span class="math inline">\(U\)</span>及其对应的特征值对角矩阵<span class="math inline">\(\Lambda\)</span>使得拉普拉斯矩阵<span class="math inline">\(L\)</span>满足<span class="math inline">\(U^TLU=\Lambda\)</span>。其中<span class="math inline">\(U=[\vec u_1, \vec u_2,..., \vec u_n]\)</span>中的<span class="math inline">\(\vec u_i\)</span>可以视为离散的傅里叶基，而所对应的特征值则为频率。如果每个结点的特征（简单起见，每个结点只有一维特征）组成的向量<span class="math inline">\(\vec x=[x_1,x_2,...,x_n]\)</span>，那么如果我们将其转化为频域向量<span class="math inline">\(\vec y=U^T\vec x\)</span>，正如我们在第一节、第二节提到的方式一样。然后就可以在频域中操作，再转换为时域即可。</p><h2 id="图的频域分量">图的频域分量</h2><p>在讲图（Graph）之前，我们可以先聊聊图像（Image）。图像通常由一个矩阵表示，矩阵中的每一个值为像素点的值。根据通道数，图像又可以分为彩色图像和灰度图像。</p><p>事实上图像它也可表示称图的结构。如下图所示<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://distill.pub/2021/gnn-intro/](https://distill.pub/2021/gnn-intro/)">[11]</span></a></sup>：</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-8.png"></p><p>其中，每个像素与上下左右对角线相邻接。</p><p>为了方便可视化图数据的频域分量，我们使用上述图数据的结构，即讲图像转化为图。具体执行如下操作<sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="[https://distill.pub/2021/understanding-gnns/#learning](https://distill.pub/2021/understanding-gnns/#learning)">[12]</span></a></sup>：</p><ul><li>step1. 选择(10, 10)尺寸的图像结构，</li><li>step2. 根据每个像素与上下左右对角线相邻接的规则构造邻接矩阵A, shape: (100, 10)</li><li>step3. 根据邻接矩阵A构造拉普拉斯矩阵L, shape: (100, 100)</li><li>step4. 对拉普拉斯矩阵进行对角化, 求得特征基矩阵U, shape(100, 100)，对角矩阵P, shape(100, 100)，注意上述2个矩阵均按特征值的大小升序排序。</li><li>step5. 取特征值大小前10的特征基，每个特征基重构为图像结构Image, shape(10, 10)，进行可视化</li></ul><p>经过以上的操作后，最后的结果如下图所示，特征值按从小到大排序。这就是每个频率所对应的分量。可以看到图像一开始是纯色的，后来开始出现了类似波形的渐变过程，随着频率的增加，其波形也越复杂。</p><p><img src="https://imagehost.vitaminz-image.top/gnn-note-9.png"></p><p>以下为实现代码</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getLaplacianOfImage</span>(<span class="hljs-params">M, N</span>):<br>    idx = np.array([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M * N)]).reshape((M, N))<br>    tmp = np.ones((M+<span class="hljs-number">2</span>, N+<span class="hljs-number">2</span>), dtype=np.int32) * M * N<br>    tmp[<span class="hljs-number">1</span>:M+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>:N+<span class="hljs-number">1</span>] = idx<br>    <span class="hljs-comment"># print(tmp)</span><br>    directH = [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]<br>    directV = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>    A = np.zeros((M * N + <span class="hljs-number">1</span>, M * N + <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, M+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> h, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(directH, directV):<br>                A[tmp[i, j], tmp[i+h, j+v]] = <span class="hljs-number">1</span><br>    A = A[<span class="hljs-number">0</span>:M*N, <span class="hljs-number">0</span>:M*N]<br>    L = np.diag(np.<span class="hljs-built_in">sum</span>(A, axis=<span class="hljs-number">1</span>)) - A<br>    <span class="hljs-keyword">return</span> L<br><br>M, N = <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br>L = getLaplacianOfImage(M, N)<br><span class="hljs-comment"># print(L)</span><br><span class="hljs-comment"># print(np.prod(L == L.T))</span><br><span class="hljs-comment"># plt.imshow(L)</span><br>eigenvalue, featurevector = np.linalg.eigh(L)<br>eigenbase = featurevector.T<br><br>min_val, max_val = np.<span class="hljs-built_in">min</span>(eigenbase), np.<span class="hljs-built_in">max</span>(eigenbase)<br>plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">9</span>):<br>    fv = eigenbase[i].reshape((M, N))<br>    plt.subplot(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, i+<span class="hljs-number">1</span>)<br>    plt.imshow(fv, vmin=min_val, vmax=max_val)<br>    plt.colorbar(fraction=<span class="hljs-number">0.045</span>)<br></code></pre></td></tr></tbody></table></figure><h2 id="总结">总结</h2><p>写该博客的起因是学习GNN时，不理解为什么使用<span class="math inline">\(L\)</span>矩阵，而不是用邻接矩阵或者其他实对称矩阵，以及在文献中<sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022.](https://arxiv.org/abs/2205.07308)">[13]</span></a></sup>看到<span class="math inline">\(L\)</span>矩阵的谱分解其实就是傅里叶变换在图领域的应用。为了深入了解这二者的关系，捡起了以前学过但又未深入理解的知识。完成这篇博客，还是很有收获的。</p><p><span id="ckwx"> </span></p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://mathworld.wolfram.com/HermitianInnerProduct.html">https://mathworld.wolfram.com/HermitianInnerProduct.html</a> <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://mathworld.wolfram.com/HilbertSpace.html">https://mathworld.wolfram.com/HilbertSpace.html</a> <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>同济大学数学系．高等数学 [M]，第七版，高等教育出版社，2014-07-04 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1za411F76U?spm_id_from=333.337.search-card.all.click&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45">https://www.bilibili.com/video/BV1za411F76U?spm_id_from=333.337.search-card.all.click&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45</a> <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1ys411472E?p=14&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45">https://www.bilibili.com/video/BV1ys411472E?p=14&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45</a> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://math.mit.edu/~gs/linearalgebra/">https://math.mit.edu/~gs/linearalgebra/</a> <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><a href="https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis">https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis</a> <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span><a href="https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis">https://en.wikipedia.org/wiki/Discrete_Fourier_transform</a> <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span><a href="https://en.wikipedia.org/wiki/DFT_matrix">https://en.wikipedia.org/wiki/DFT_matrix</a> <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf">https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a> <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span><a href="https://distill.pub/2021/gnn-intro/">https://distill.pub/2021/gnn-intro/</a> <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span><a href="https://distill.pub/2021/understanding-gnns/#learning">https://distill.pub/2021/understanding-gnns/#learning</a> <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span><a href="https://arxiv.org/abs/2205.07308">Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022.</a> <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>谱图理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>频域</tag>
      
      <tag>谱图理论</tag>
      
      <tag>傅里叶变换</tag>
      
      <tag>线性代数</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文速读&lt;一&gt;：关系抽取与提示学习</title>
    <link href="/2022/07/16/kg/lun-wen-su-du-1-guan-xi-chou-qu/"/>
    <url>/2022/07/16/kg/lun-wen-su-du-1-guan-xi-chou-qu/</url>
    
    <content type="html"><![CDATA[<h1 id="论文速读关系抽取与提示学习">论文速读&lt;一&gt;：关系抽取与提示学习</h1><p>论文速读系列为对论文的核心思想进行快速抓取，仅记录论文的Key Idea。本期为论文速读系列的第一期，选取了5篇近期知识图谱领域的关系抽取相关论文。</p><h2 id="key-idea">Key Idea</h2><h3 id="enriching-pre-trained-language-model-with-entity-information-for-relation-classification.1"><em>Enriching pre-trained language model with entity information for relation classification.</em><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Shanchan Wu and Yifan He. 2019. Enriching pre-trained language model with entity information for relation classification. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 2361–2364.](https://dl.acm.org/doi/abs/10.1145/3357384.3358119)">[1]</span></a></sup></h3><p><img src="https://imagehost.vitaminz-image.top/SEofZJU-1.png"></p><p>R-Bert模型的框架如上图所示，其主要思路就是对每个句子的关系的头部放一个分类头，并在2个实体出使用特殊标记，放进预训练好后的Bert中，在输出中，实体所对应的embedding做一个平均值，和分类头的embedding分别经过3个MLP，最后做一个拼接，输出到softmax中去，进行分类。</p><h3 id="relation-classification-with-entity-type-restriction.">Relation classification with entity type restriction.</h3><p><img src="https://imagehost.vitaminz-image.top/SEofZJU-2.png"></p><p>该论文事实上是提出了一个算法无关的流程。</p><p>传统的关系抽取是将句子、实体以及实体类型一起丢进一个分类器中，然后输出关系的类别。但假如根据实体的类型可以一定程度上的筛掉一些不可能的关系。因此区别于传统的做法，它首先将实体丢进一个分类器，然后获得其类别之后，根据实体的类型去训练专用的分类器，最后句子一起输入到这个专用分类器中，输出对应的类别。</p><h3 id="ptr-prompt-tuning-with-rules-for-text-classification.">Ptr: Prompt tuning with rules for text classification.</h3><p><img src="https://imagehost.vitaminz-image.top/SEofZJU-3.png"></p><p>该论文提出了一种提示学习的方法对文本进行分类。提示学习就是在Fine Tuning的时候加一个模板化的提示，这样可以使得原问题更贴近于自然语言处理的问题，从而更贴合预训练的自然语言模型。</p><p>该论文的模式来源于推理规则，他在Fine Tuning时，在句子的结尾增添提示。给出句子的实体，但对实体的类型和实体间的关系做一个mask。这样可以使得模型能将实体与对应的类型结合起来，推理出他们之间的关系。</p><h3 id="summarization-as-indirect-supervision-for-relation-extraction">Summarization as Indirect Supervision for Relation Extraction</h3><p><img src="https://imagehost.vitaminz-image.top/SEofZJU-4.png"></p><p>该论文将摘要模型用于关系抽取。</p><p>若想要将摘要模型应用在句子的关系抽取上，需要将关系抽取问题转化为摘要问题。直觉上来讲，上游模型的特点和下游任务的关系月紧密，它的效果也会越好。</p><p>和通常的提示学习想法类似，它也是对原句子进行模板化改造，只不过将句子转换为一个段落而已。如上图所示，所谓的段落就是，加上“主语是。。。”，“谓语是。。。”这样的信息。</p><p>由于摘要模型输入是一个段落，输出是一个句子，而不是一个标签。因此输出的标签也应当做一个改造。改造的方式也很简单，比如"city of birth"就改在为"subj was born in the city obj"。需要注意的是，主语放在句子开头，宾语放在句子结尾。这是方便后面的预测。</p><p>其预测过程是构造一棵字典树，由于所有的句子主语放在开头，所以他们有同一个根结点。从根据结点出发，每次遇到一个分叉就使用decoder进行预测，给出每个分叉的概率。最后每个类别的概率，就是跟结点到叶结点的路上所有概率的乘积。选择最大的就是输出。</p><h3 id="prefix-tuning-optimizing-continuous-prompts-for-generation.">Prefix-tuning: Optimizing continuous prompts for generation.</h3><p><img src="https://imagehost.vitaminz-image.top/SEofZJU-5.png"></p><p>提示学习所用的提示模板往往是一个个离散的词。该文章提出了一种将离散词嵌入到连续空间中去，比如使用2个向量，使其起到提示的效果。</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358119">Shanchan Wu and Yifan He. 2019. Enriching pre-trained language model with entity information for relation classification. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 2361–2364.</a> <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/2105.08393">Shengfei Lyu and Huanhuan Chen. 2021. Relation classification with entity type restriction. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 390–395, Online. Association for Computational Linguistics.</a> <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/2105.11259">Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. Ptr: Prompt tuning with rules for text classification. arXiv preprint arXiv:2105.11259.</a> <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://arxiv.org/abs/2205.09837">Lu K, Hsu I, Zhou W, et al. Summarization as Indirect Supervision for Relation Extraction[J]. arXiv preprint arXiv:2205.09837, 2022.</a> <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://arxiv.org/abs/2101.00190">Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics.</a> <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>知识图谱</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图</tag>
      
      <tag>知识图谱</tag>
      
      <tag>关系抽取</tag>
      
      <tag>提示学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手写一个编译器</title>
    <link href="/2022/06/20/compiler/dong-shou-xie-yi-ge-bian-yi-qi/"/>
    <url>/2022/06/20/compiler/dong-shou-xie-yi-ge-bian-yi-qi/</url>
    
    <content type="html"><![CDATA[<h1 id="toyc语言">ToyC语言</h1><p>本项目用于学习编译原理。</p><p>将参照龙书版本的《编译原理》，</p><p>以及LLVM的编译器制作教程：https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.html</p><p>制作一个完整的编译器前端。</p><h2 id="编译器的结构">编译器的结构</h2><p><img src="https://imagehost.vitaminz-image.top/ToyC-1.png"></p><center>图1：摘自《Compilers Principles, Techniques &amp; Tools》第二版Figure 2.3</center><h3 id="uml">UML</h3><p><img src="https://imagehost.vitaminz-image.top/UML.png"></p><center>图2：UML</center><h3 id="词法分析器lexer">词法分析器(Lexer)</h3><h4 id="设计token">设计Token</h4><p>Token主要分为: 多字符保留字、标识符、数字以及其余单个字符。</p><ul><li>多字符保留字：<ul><li>控制流语句：if, else, do, while, break</li><li>布尔运算：true, false, &amp;&amp;, ||</li><li>比较运算：&gt;=, &lt;=, ==, !=</li><li>变量类型：int, float, bool, char</li></ul></li><li>标识符：<ul><li>正则表达式：[_a-zA-Z][_a-zA-Z0-9]*</li></ul></li><li>数字：<ul><li>整型正则表达式：[0-9]+</li><li>浮点型正则表达式：[0-9]+.[0-9]*</li></ul></li><li>其余单个字字符</li></ul><h4 id="识别算法">识别算法</h4><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs"><br></code></pre></td></tr></tbody></table></figure><h4 id="测试">测试</h4><p>输入：字符串 输出：按序输出token流，每个token占一行</p><p>词法分析器的步骤可用如下伪代码表示：</p><figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c">Input:  <br>Output: <br>cache = <span class="hljs-string">' '</span><br>Scan() {<br>    <span class="hljs-keyword">do</span>{<br>        <span class="hljs-keyword">if</span> cache != 空格、换行、制表符<br>            <span class="hljs-keyword">break</span>;<br>    }<span class="hljs-keyword">while</span>(read(cache));<br><br>    t = readKey(cache)     <span class="hljs-comment">// 识别保留字</span><br>    t = readNumber(cache); <span class="hljs-comment">// 识别实数</span><br>    t = <br><br>    <span class="hljs-keyword">return</span> t;<br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="语法分析器parser">语法分析器(Parser)</h3><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs c++">PROGRAM -&gt; BLOCK<br>BLOCK -&gt; <span class="hljs-string">'{'</span> DECLS STMTS <span class="hljs-string">'}'</span><br>DECLS -&gt; DECLS DECL<br>    -&gt; eps<br>DECL -&gt; TYPE id<span class="hljs-number">'</span>;'<br>TYPE-&gt; basic DIMS<br>DIMS-&gt; <span class="hljs-string">'['</span>DIMS<span class="hljs-number">'</span>]'<br>-&gt; eps<br>STMTS-&gt; STMTS STMT<br>    -&gt; eps<br><br>STMT-&gt; ASSIGN<span class="hljs-number">'</span>;'<br>-&gt; <span class="hljs-keyword">if</span> ( BOOL ) STMT<br>        -&gt; <span class="hljs-keyword">if</span> ( BOOL ) STMT <span class="hljs-keyword">else</span> STMT<br>        -&gt; <span class="hljs-keyword">while</span> ( BOOL ) STMT<br>        -&gt; <span class="hljs-keyword">do</span> STMT <span class="hljs-keyword">while</span> ( BOOL )<br>        -&gt; <span class="hljs-keyword">break</span><span class="hljs-number">'</span>;'<br>        -&gt; BLOCK<br>ASSIGN  -&gt; id OFFSET = BOOL<br>OFFSET  -&gt; [ BOOL ] OFFSET<br>        -&gt; eps<br>            <br>BOOL-&gt; BOOL <span class="hljs-string">"||"</span> JOIN<br>        -&gt; JOIN<br>JOIN-&gt; JOIN <span class="hljs-string">"&amp;&amp;"</span> EQAULITY<br>        -&gt; EQUALITY<br>EQUALITY-&gt; EQUALITY <span class="hljs-string">"=="</span> CMP<br>        -&gt; EQUALITY <span class="hljs-string">"!="</span> CMP<br>        -&gt; CMP<br>CMP-&gt; EXPR &lt; EXPR<br>        -&gt; EXPR &lt;= EXPR<br>        -&gt; EXPR &gt;= EXPR<br>        -&gt; EXPR &gt; EXPR<br>        -&gt; EXPR<br>EXPR-&gt; EXPR + TERM<br>        -&gt; EXPR - TERM<br>        -&gt; TERM<br>TERM-&gt; TERM * UNARY<br>        -&gt; TERM / UNARY<br>        -&gt; UNARY<br>UNARY-&gt; <span class="hljs-string">'!'</span> UNARY<br>        -&gt; <span class="hljs-string">'-'</span> UNARY<br>        -&gt; FACTOR<br>FACTOR-&gt; ( BOOL )<br>        -&gt; id OFFSET<br>        -&gt; number<br>        -&gt; real<br>        -&gt; <span class="hljs-literal">true</span><br>        -&gt; <span class="hljs-literal">false</span><br></code></pre></td></tr></tbody></table></figure><h3 id="符号表symbol-table">符号表(Symbol Table)</h3><p><img src="https://imagehost.vitaminz-image.top/ToyC-2.png" style="zoom:50%;"></p><center>图2：符号表示意图</center><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++">PROGRAM -&gt; {top = null;} BLOCK<br>BLOCK -&gt; <span class="hljs-string">'{'</span> <br>{ saved = top;<span class="hljs-comment">// 保留现场，saved</span><br>  top = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Scope</span>(top); } <span class="hljs-comment">// 碰到块建立符号表，top指向当前块符号表</span><br>DECLS DECL <br>{ top = saved; }<span class="hljs-comment">// 恢复现场，</span><br><span class="hljs-string">'}'</span><br>DECLS -&gt; DECLS DECL<br>      -&gt; eps<br>DECL  -&gt; TYPE id<span class="hljs-number">'</span>;<span class="hljs-string">' { s = new Symbol(id);// </span><br><span class="hljs-string">  s.type = TYPE.lexeme;</span><br><span class="hljs-string">  top.put(id.lexeme, s); }</span><br><span class="hljs-string">TYPE  -&gt; basic {DIMS.type = basic; } </span><br><span class="hljs-string"> DIMS {TYPE.lexeme = DIMS.type; }</span><br><span class="hljs-string">DIMS  -&gt; '</span>[<span class="hljs-string">'num'</span>]<span class="hljs-string">' DIMS { Array.sz = num * Array.sz;</span><br><span class="hljs-string">      DIMS.type = Array; }</span><br><span class="hljs-string">      -&gt; eps { Array.sz = 1; </span><br><span class="hljs-string">               Array.type = Dims.type; }</span><br><span class="hljs-string">STMTS -&gt; STMTS STMT </span><br><span class="hljs-string">      -&gt; eps</span><br><span class="hljs-string">STMT  -&gt; BLOCK</span><br><span class="hljs-string">STMT  -&gt; .... &gt; ... id { s = top.get(id.lexeme); } ....</span><br></code></pre></td></tr></tbody></table></figure><p>单元测试：</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++">PROGRAM -&gt; BLOCK<br>BLOCK -&gt; <span class="hljs-string">'{'</span> DECLS STMTS <span class="hljs-string">'}'</span><br>DECLS -&gt; DECLS DECL<br>    -&gt; eps<br>DECL -&gt; TYPE id <span class="hljs-string">';'</span><br>TYPE-&gt; basic DIMS<br>DIMS-&gt; <span class="hljs-string">'['</span>num<span class="hljs-number">'</span>]<span class="hljs-string">'DIMS</span><br><span class="hljs-string">-&gt; eps</span><br><span class="hljs-string">STMTS-&gt; STMTS STMT</span><br><span class="hljs-string">    -&gt; eps</span><br><span class="hljs-string">STMT-&gt; BLOCK</span><br><span class="hljs-string">    -&gt; FACTOR '</span>;'<br>FACTOR-&gt; id<br></code></pre></td></tr></tbody></table></figure><h3 id="中间代码intermediate-code">中间代码(Intermediate Code)</h3><h4 id="表达式的计算">表达式的计算</h4><p><img src="https://imagehost.vitaminz-image.top/ToyC-3.png" style="zoom: 25%;"></p><p><img src="https://imagehost.vitaminz-image.top/ToyC-5.png" style="zoom: 25%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c++">PROGRAM -&gt; STMTS<br>STMTS-&gt; STMTS STMT<br>    -&gt; eps<br>STMT-&gt; ASSIGN<span class="hljs-number">'</span>;'<br>ASSIGN  -&gt; id OFFSET = BOOL<br>OFFSET  -&gt; [ BOOL ] OFFSET<br>        -&gt; eps<br><br>BOOL-&gt; BOOL <span class="hljs-string">"||"</span> JOIN<br>        -&gt; JOIN<br>JOIN-&gt; JOIN <span class="hljs-string">"&amp;&amp;"</span> EQAULITY<br>        -&gt; EQUALITY<br>EQUALITY-&gt; EQUALITY <span class="hljs-string">"=="</span> CMP<br>        -&gt; EQUALITY <span class="hljs-string">"!="</span> CMP<br>        -&gt; CMP<br>CMP-&gt; EXPR &lt; EXPR<br>        -&gt; EXPR &lt;= EXPR<br>        -&gt; EXPR &gt;= EXPR<br>        -&gt; EXPR &gt; EXPR<br>EXPR-&gt; EXPR + TERM<br>        -&gt; EXPR - TERM<br>        -&gt; TERM<br>TERM-&gt; TERM * UNARY<br>        -&gt; TERM / UNARY<br>        -&gt; UNARY<br>UNARY-&gt; <span class="hljs-string">'!'</span> UNARY<br>        -&gt; <span class="hljs-string">'-'</span> UNARY<br>        -&gt; FACTOR<br>FACTOR-&gt; ( BOOL )<br>        -&gt; OFFSET<br>        -&gt; number<br>        -&gt; real<br>        -&gt; <span class="hljs-literal">true</span><br>        -&gt; <span class="hljs-literal">false</span><br>        -&gt; id OFFSET<br></code></pre></td></tr></tbody></table></figure><p>输入</p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs txt">a = b + 1;<br>c = a + 32 * 43 - b / ( g - 2);<br>d = e + f;<br>c = -e[3];<br>c[33+34+sd3*c] = de*c + c2;<br>f[c[2*d]+4] = df + de[23-s[f]];<br>s[m][n][o] = -d[3][x];<br>a = !a || s &amp;&amp; (c || d) || !f &amp;&amp; kk ;<br>b = (a + c) &gt; (b * 2 - 1) || a &lt; b &amp;&amp; c;<br></code></pre></td></tr></tbody></table></figure><p>输出</p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs txt">a = b + 1<br>L3:t1 = 32 * 43<br>t2 = a + t1<br>t3 = g - 2<br>t4 = b / t3<br>c = t2 - t4<br>L4:d = e + f<br>L5:t5 = 3 * 10<br>t6 = e[t5]<br>c = -t6<br>L6:t7 = 33 + 34<br>t8 = sd3 * c<br>t9 = t7 + t8<br>t10 = t9 * 10<br>t11 = c[t10]<br>t12 = de * c<br>t13 = t12 + c2<br>c[t11] = t13<br>L7:t14 = 2 * d<br>t15 = t14 * 10<br>t16 = c[t15]<br>t17 = t16 + 4<br>t18 = t17 * 10<br>t19 = f[t18]<br>t20 = f * 10<br>t21 = s[t20]<br>t22 = 23 - t21<br>t23 = t22 * 10<br>t24 = de[t23]<br>t25 = df + t24<br>f[t19] = t25<br>L8:t26 = m * 10<br>t27 = n * 10<br>t28 = t26 + t27<br>t29 = o * 10<br>t30 = t28 + t29<br>t31 = s[t30]<br>t32 = 3 * 10<br>t33 = x * 10<br>t34 = t32 + t33<br>t35 = d[t34]<br>t36 = -t35<br>s[t31] = t36<br>L9:if False a goto L13<br>t37 = 23 * 10<br>t38 = f * 10<br>t39 = t37 + t38<br>t40 = sp[t39]<br>if False t40 goto L14<br>if c goto L13<br>if d goto L13<br>goto L14<br>L14:if f goto L11<br>if kk goto L13<br>goto L11<br>L13:t41 = true<br>goto L12<br>L11:t41 = false<br>L12:a = t41<br>L10:t42 = a + c<br>t43 = b * 2<br>t44 = t43 - 1<br>if t42 &gt; t44 goto L17<br>if False a &lt; b goto L15<br>if c goto L17<br>goto L15<br>L17:t45 = true<br>goto L16<br>L15:t45 = false<br>L16:b = t45<br>L2:<br><br></code></pre></td></tr></tbody></table></figure><h4 id="控制流语句的中间代码">控制流语句的中间代码</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++">PROGRAM -&gt; BLOCK<br>BLOCK -&gt; <span class="hljs-string">'{'</span> STMTS <span class="hljs-string">'}'</span><br>STMTS-&gt; STMTS STMT<br>    -&gt; eps<br><br>STMT-&gt; ASSIGN<span class="hljs-number">'</span>;'<br>-&gt; <span class="hljs-keyword">if</span> ( BOOL ) STMT<br>        -&gt; <span class="hljs-keyword">if</span> ( BOOL ) STMT <span class="hljs-keyword">else</span> STMT<br>        -&gt; <span class="hljs-keyword">while</span> ( BOOL ) STMT<br>        -&gt; <span class="hljs-keyword">do</span> STMT <span class="hljs-keyword">while</span> ( BOOL )<br>        -&gt; <span class="hljs-keyword">break</span><span class="hljs-number">'</span>;'<br>        -&gt; BLOCK<br>ASSIGN  -&gt; id OFFSET = BOOL<br>OFFSET  -&gt; [ BOOL ]<br>        -&gt; eps<br>            <br>BOOL-&gt; BOOL <span class="hljs-string">"||"</span> JOIN<br>        -&gt; JOIN<br>JOIN-&gt; JOIN <span class="hljs-string">"&amp;&amp;"</span> EQAULITY<br>        -&gt; EQUALITY<br>EQUALITY-&gt; EQUALITY <span class="hljs-string">"=="</span> CMP<br>        -&gt; EQUALITY <span class="hljs-string">"!="</span> CMP<br>        -&gt; CMP<br>CMP-&gt; EXPR &lt; EXPR<br>        -&gt; EXPR &lt;= EXPR<br>        -&gt; EXPR &gt;= EXPR<br>        -&gt; EXPR &gt; EXPR<br><br></code></pre></td></tr></tbody></table></figure><p>输入</p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs txt">{<br>    if ((a + c) &gt; (b * 2 - 1) || a &lt; b &amp;&amp; c){<br>        a = b * 2 - c + (b + 2 * d);<br>    }<br>    while (a &gt; 3){<br>        a = a + 1;<br>        do a = a + 3; while( b &gt; 2);<br>        if (c - 3){<br>            b = c + 5;<br>            d = a + 3;<br>        }<br>        else {<br>            c = 3;<br>            break;<br>        }<br>    }<br>    while (c &lt; b){<br>        a = 3;<br>    }<br>}<br></code></pre></td></tr></tbody></table></figure><p>输出</p><figure class="highlight vbnet"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">L1:</span>t1 = a + c<br>t2 = b * <span class="hljs-number">2</span><br>t3 = t2 - <span class="hljs-number">1</span><br><span class="hljs-keyword">if</span> t1 &gt; t3 <span class="hljs-keyword">goto</span> L5<br><span class="hljs-keyword">if</span> <span class="hljs-literal">False</span> a &lt; b <span class="hljs-keyword">goto</span> L3<br><span class="hljs-keyword">if</span> c <span class="hljs-keyword">goto</span> L5<br><span class="hljs-keyword">goto</span> L3<br><span class="hljs-symbol">L5:</span>L4:t4 = b * <span class="hljs-number">2</span><br>t5 = t4 - c<br>t6 = <span class="hljs-number">2</span> * d<br>t7 = b + t6<br>a = t5 + t7<br><span class="hljs-symbol">L3:</span><span class="hljs-keyword">if</span> <span class="hljs-literal">False</span> a &gt; <span class="hljs-number">3</span> <span class="hljs-keyword">goto</span> L6<br><span class="hljs-symbol">L7:</span>a = a + <span class="hljs-number">1</span><br><span class="hljs-symbol">L8:</span>a = a + <span class="hljs-number">3</span><br><span class="hljs-symbol">L10:</span><span class="hljs-keyword">if</span> b &gt; <span class="hljs-number">2</span> <span class="hljs-keyword">goto</span> L8<br><span class="hljs-keyword">goto</span> L9<br><span class="hljs-symbol">L9:</span><span class="hljs-keyword">if</span> <span class="hljs-literal">False</span> c - <span class="hljs-number">3</span> <span class="hljs-keyword">goto</span> L12<br><span class="hljs-symbol">L11:</span>b = c + <span class="hljs-number">5</span><br><span class="hljs-symbol">L13:</span>d = a + <span class="hljs-number">3</span><br><span class="hljs-keyword">goto</span> L3<br><span class="hljs-symbol">L12:</span>c = <span class="hljs-number">3</span><br><span class="hljs-symbol">L14:</span><span class="hljs-keyword">goto</span> L6<br><span class="hljs-keyword">goto</span> L3<br><span class="hljs-symbol">L6:</span><span class="hljs-keyword">if</span> <span class="hljs-literal">False</span> c &lt; b <span class="hljs-keyword">goto</span> L2<br><span class="hljs-symbol">L15:</span>a = <span class="hljs-number">3</span><br><span class="hljs-keyword">goto</span> L6<br><span class="hljs-symbol">L2:</span><br><br></code></pre></td></tr></tbody></table></figure><h4 id="总结">总结</h4><h2 id="附录">附录</h2>]]></content>
    
    
    <categories>
      
      <category>编译原理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>编译原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
