<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>上线测试 | 闲渔</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.0.0">
<!-- hexo-inject:begin --><!-- hexo-inject:end --><style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">上线测试</h1><a id="logo" href="/.">闲渔</a><p class="description">VITAMINZL</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">上线测试</h1><div class="post-meta">2022-08-20<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%EF%BC%88Graph%EF%BC%89%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%91%E5%9F%9F"><span class="toc-number">1.</span> <span class="toc-text">图（Graph）数据的频域</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E7%A7%AF%E4%B8%8E%E5%9F%BA"><span class="toc-number">1.1.</span> <span class="toc-text">内积与基</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2"><span class="toc-number">1.2.</span> <span class="toc-text">傅里叶变换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-number">1.3.</span> <span class="toc-text">特征值与特征向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5"><span class="toc-number">1.4.</span> <span class="toc-text">拉普拉斯矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%9A%84%E9%A2%91%E5%9F%9F%E5%88%86%E9%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">图的频域分量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.7.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div><div class="post-content"><h1 id="图（Graph）数据的频域"><a href="#图（Graph）数据的频域" class="headerlink" title="图（Graph）数据的频域"></a>图（Graph）数据的频域</h1><p>图神经网络（GNN）是近年来愈发火热，用以对图（Graph）数据进行特征提取以及各种下游任务。注意这里的图（Graph）应和图像（Image）区分，图是一种由点集与边集组成的数据结构，常记作$\mathcal{G}=(\mathcal{V,E})$。以下是我学习谱图理论时的一些记录。本文先先从基变换的角度入手，说明了标准正交基的性质，并从实数域拓展到了复数域，接着从基变换的角度阐述了傅里叶变换的原理，然后从特征值与特征向量入手，通过分析拉普拉斯算子的特征向量，理解其与傅里叶变换的等价性。最后使用图的数据结构表示图像，可视化其特征向量，直观感受低频到高频分量的差异。本人才疏学浅，错误难免，欢迎交流指正。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="内积与基"><a href="#内积与基" class="headerlink" title="内积与基"></a>内积与基</h2><p>我们首先回顾一下线性代数的知识。本科第一次线性代数的时候，都是以繁杂的计算与证明为主，未曾有更直观、直觉的方式去理解。这里我想从线性变换的角度，讲述线性代数中与本文内容相关的知识。</p>
<p>我们定义实数向量$\vec x=[x_1, x_2,…,x_n]^T$与实数向量的$\vec x=[x_1, x_2,…,x_n]^T$内积为$&lt;\vec x,\vec y&gt;=\sum x_iy_i$。这是高中就学过的知识。但这里，我需要对向量的内积做一些扩充，即复数向量内积的定义。根据hermitian内积的定义[[1]]<a target="_blank" rel="noopener" href="https://mathworld.wolfram.com/HermitianInnerProduct.html" title="Hermitian内积的定义">1</a>，复平面内积定义为$&lt;\vec x,\vec y&gt;=\sum x_i\bar y_i$。</p>
<p>如果我们将向量内积扩展到函数空间，我们可以将函数视为无限长的向量，如$f(x)=[f(x_1),f(x_2),…]$，可以定义函数的内积[[2]]<a target="_blank" rel="noopener" href="https://mathworld.wolfram.com/HilbertSpace.html" title="函数内积">2</a>$&lt;f,g&gt;=\int_a^b f(x)g(x)dx$。若函数在复平面上，则可定义为$&lt;f,g&gt;=\int_a^b f(x)\overline{g(x)}dx$（有时共轭会放在左边）。</p>
<p>然后我们来回顾一下向量空间中的基。由一组线性无关的向量可以张成一个向量空间，空间中的任意向量都可以使用这一组向量的线性组合表示。这组向量称作基向量。如果基向量构成的矩阵$A=[\vec\alpha_1,\vec\alpha_2,…,\vec\alpha_n]^T$满足$AA^T=E$，则这组基向量称为正交基，若还满足$||\vec\alpha_i||=1,(i=1,2,…,n)$，则称为标准正交基。如，二维向量$[1,0]^T,[0,1]^T$构成一组标准正交基。在复平面中[[3]]<a target="_blank" rel="noopener" href="https://math.mit.edu/~gs/linearalgebra/" title=" 《线性代数导论》">6</a>，转置被描述为$(A^H)<em>{ij}=\overline{A</em>{ji}}$，相比实数域，处了转置还要做一次共轭，称为共轭转置。那么正交基构成的矩阵$A$应满足$AA^H=E$。</p>
<p>假如有标准正交基$\vec e_1, \vec e_2,…,\vec e_n$，若该空间中向量<br>$$<br>\vec v=w_1\vec e_1+w_2\vec e_2+,…+w_n\vec e_n=[\vec e_1,\vec e_2,…, \vec e_n][w_1,w_2,…,w_m]^T<br>$$<br>则<br>$$<br>[w_1,w_2,…,w_m]^T=[\vec e_1,\vec e_2,… ,\vec e_n]^{-1}\vec v=[\vec e_1,\vec e_2,… ,\vec e_n]^{T}\vec v<br>$$<br>可以看到，若想获得一组某向量在一组标准正交基上的表示，只需要让该向量与这组正交基做内积即可。</p>
<p>函数也存在着基的概念，若函数$f(x)=a_ng(x)+b_nh(x)$。那么$g(x)$和$h(x)$就是$f(x)$的基。若内积$&lt;g,h&gt;=0$，$g$和$h$是一组正交基。</p>
<h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><p><img src="https://imagehost.vitaminz-image.top/gnn-note-5.png"></p>
<p>在学习傅里叶变换时，想必类似上面的图大家已经见过很多次了，这里我们暂且不从几何的角度去解释，从基的角度去阐述。大多数教材都会先从傅里叶级数入手，周期函数可以表示为许多正弦信号的叠加，有如下形式<br>$$<br>f(t)=\frac{a_0}{2}+\sum_{n=1}^{+\infin} [a_n\sin(\frac{n\pi}{l} t )+b_n\cos({\frac{n\pi}{l} t})]<br>$$<br>其中，$n$是整数，$l$为半周期。$1,\sin(n\omega t ),\cos({n\omega t})$可以看作是$f(t)$许许多多相互正交的基。这是因为[[3]](#ckwx)：$\int^{\pi}<em>{-\pi}\cos nxdx=0$,$\int^{\pi}</em>{-\pi}\sin nxdx=0$, $\int^{\pi}<em>{-\pi}\cos n_1x\sin n_2xdx=0$, $\int^{\pi}</em>{-\pi}\cos n_1x\cos n_2xdx=0$,$\int^{\pi}_{-\pi}\sin n_1x\sin n_2xdx=0$。</p>
<p>如果使用Euler公式替换，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       可以转换为如下形式<br>$$<br>f(t)=\sum^{+\infin}<em>{n=-\infin}c_ne^{i\omega t}<br>$$<br>其中，$c_n=\frac{1}{l}\int</em>{-l}^{l}f(t)e^{-i\omega t}dt$。$l$为半周期，$\omega=\frac{n\pi}{l}$，即频率（有些地方会将$\omega$视作角速度，指数项写作$e^{2i\pi\omega t}$）。这里$e^{i\omega t}$也是许多的标准正交基，这是因为$\int e^{i\omega_1 t}e^{-i\omega_2 t}=0$，$||e^{i\omega}||=1$。那么当$l\rightarrow +\infin$时，$c_n$就是傅里叶变换的形式了。即<br>$$<br>\mathcal{F}[f(t)]=\hat f(\omega)=\int_{-\infin}^{+\infin}f(t)e^{-i\omega t}dt<br>$$<br>所以像函数$\hat f(\omega)$其实就是$\omega$对应的傅里叶基$e^{i\omega t}$的系数。上述式子也可以看作是$f(t)$与正交基$e^{i\omega t}$的内积，在上一节中提到了，若某个向量希望使用某组正交基来表示，拿就让这个向量和这组标准正交基做内积，标准正交基就像一个筛子，把基方向的分量提取出来了。</p>
<p>在计算机中，使用离散傅里叶变换（DFT）的算法计算：被描述为[[8]]<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform" title="离散傅里叶变换">8</a><br>$$<br>X_k=\sum_{n=0}^{N-1}x_n \cdot e^{-\frac{i2\pi}{N}kn}<br>$$<br>实际计算时如快速傅里叶变换（FFT），使用如下DFT矩阵[[9]]<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/DFT_matrix" title="DFT矩阵">9</a>与向量$\vec x=[x_1,x_2,…,x_n]^T$相乘。<br>$$<br>W = \frac{1}{\sqrt{N}} \begin{bmatrix}<br>1&amp;1&amp;1&amp;1&amp;\cdots &amp;1 \<br>1&amp;\omega&amp;\omega^2&amp;\omega^3&amp;\cdots&amp;\omega^{N-1} \<br>1&amp;\omega^2&amp;\omega^4&amp;\omega^6&amp;\cdots&amp;\omega^{2(N-1)}\ 1&amp;\omega^3&amp;\omega^6&amp;\omega^9&amp;\cdots&amp;\omega^{3(N-1)}\<br>\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\<br>1&amp;\omega^{N-1}&amp;\omega^{2(N-1)}&amp;\omega^{3(N-1)}&amp;\cdots&amp;\omega^{(N-1)(N-1)}<br>\end{bmatrix}<br>$$<br>其中$\omega = e^{-2\pi i/N}$。易证明$WW^H=E$，即它由一组标准正交基构成。具体的计算方法使用快速傅里叶变换，可以将复杂度降至$O(n\log n)$，具体过程可以参考[[4]]<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1za411F76U?spm_id_from=333.337.search-card.all.click&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45" title="快速傅里叶变换（FFT）">4</a>。</p>
<h2 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h2><p>简单地回顾一些特征值与特征向量的定义，设有矩阵$A$，若存在$\lambda,\vec x$，使得$A\vec x=\lambda \vec x$，则$\lambda$称为$A$的特征值，$\vec x$则称为特征向量，$\lambda$的值可以不止一个，同一个$\lambda$可以对应多个线性无关的$\vec x$。如果想了解特征值与特征向量的几何解释，可以参考[[5]]<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ys411472E?p=14&amp;vd_source=3eafcac5a31e0009a6433cea9bc7ab45" title="特征值与特征向量的几何解释">5</a>，这不是本文的重点。</p>
<p>接下啦我想说明的是一种特殊的矩阵：实对称矩阵，即由实数组成的对称矩阵（若$A=A^T$则称$A$为对称矩阵）。该矩阵有很多优良的性质。首先，$N$阶是对称矩阵，具有$N$个特征值以及$N$个正交的特征向量。且实对称矩阵一定可以相似对角化，即$Q^{-1}AQ=Q^TAQ=\Lambda$。对角矩阵$\Lambda$对角线上为特征值$\lambda_1,\lambda_2,…$，分别对应$Q=[\vec q_1, \vec q_2,…]$中的特征向量$\vec q_1,\vec q_2,…$。</p>
<p>由于其$Q$由一组线性无关的正交特征向量组成，他们可以构成一组正交基，特征向量组成的基也成为特征基。我们举个相似对角化的简单应用。假如要求实对称矩阵$A$的幂次预算，如$A^{100}$。直接计算是很麻烦的，若首先将其转化为对角矩阵$Q^{-1}AQ=\Lambda$，那么$Q^{-1}AQ…Q^{-1}AQQ^{-1}AQ=Q^{-1}A^{100}Q=\Lambda^{100}$。则$A^{100}=Q\Lambda^{100}Q^{-1}$。对角矩阵的100次幂是非常容易计算的，这就使得计算量大大减小。</p>
<p>特征值与特征向量在微分方程中也有着重要应用[[6]][]。如对一个一阶微分方程$\frac{du}{dt}=\lambda u$。它的特解为$e^{\lambda t}$，通解为$u(t)=Ce^t$。假如$A$是一个常数矩阵，$\vec u=[u_1,u_2,…,u_n]$。$\frac{d\vec u}{dt}=A \vec u$，那么这就是在求解一个微分线性方程组。我们可以验证的是（将以下2式子带入原方程就可以证得），当$A\vec x= \lambda \vec x $时，$\vec u$的一个特解为$\vec u = e^{\lambda t}\vec x$，$\vec x$是一个常数向量。那么解原方程组转化为求$A\vec x= \lambda \vec x $的问题求出特征值与特征向量，最后的通解即为$C_1e^{\lambda_1 t}\vec x_1+C_2e^{\lambda_2 t}\vec x_2,….$。</p>
<p>若我们将一阶微分记作$\nabla$，则$\frac{d\vec u}{dt} = \nabla \vec u$，带入一个特解可以得到$\nabla \vec u=\nabla e^{\lambda t}\vec x = \lambda e^{\lambda t}\vec x$。我们可以将$\lambda$视为$\Delta$的特征值，$e^{\lambda t}$为特征向量。假如是二阶微分方程$\frac{d^2\vec u}{dt^2}=A \vec u$，会有所不同，若满足$A\vec x= \lambda \vec x $，其特解为$\lambda e^{i\omega t} \vec x$，其中$\omega^2=-\lambda$。同上，若我们将二阶微分算子记为$\Delta$，则有$\Delta \vec u =\Delta e^{i\omega t}\vec x= \lambda e^{i\omega t}\vec x$。那么$e^{i\omega t}\vec x$就是$\Delta$的特征向量。</p>
<h2 id="拉普拉斯矩阵"><a href="#拉普拉斯矩阵" class="headerlink" title="拉普拉斯矩阵"></a>拉普拉斯矩阵</h2><img src="https://imagehost.vitaminz-image.top/gnn-note-7.png" style="zoom: 50%;">

<p>对于如上的图数据，它的拉普拉斯矩阵如下。其计算方法是$D-A$。$D$是每个结点的度组成的对角矩阵，$A$是图的邻接矩阵。<br>$$<br>L=\left(\begin{array}{rrrrrr}<br>   2 &amp; -1 &amp;  0 &amp;  0 &amp; -1 &amp;  0\<br>  -1 &amp;  3 &amp; -1 &amp;  0 &amp; -1 &amp;  0\<br>   0 &amp; -1 &amp;  2 &amp; -1 &amp;  0 &amp;  0\<br>   0 &amp;  0 &amp; -1 &amp;  3 &amp; -1 &amp; -1\<br>  -1 &amp; -1 &amp;  0 &amp; -1 &amp;  3 &amp;  0\<br>   0 &amp;  0 &amp;  0 &amp; -1 &amp;  0 &amp;  1\<br>\end{array}\right)<br>$$<br>拉普拉斯矩阵是拉普拉斯算子在图数据中的表示方式[[10]]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85287578" title="拉普拉斯算子与拉普拉斯矩阵">10</a>，也被称为离散拉普拉斯算子。拉普拉斯算子是一个二阶微分算子，记作$\Delta f = \sum_{i=1}^n \frac {\partial^2 f}{\partial x^2_i}$，在上一节的结尾，我们其实提到了这个算子。它的特征向量其实就是傅里叶变换中的正交基。实际上，实际上函数的傅里叶变换是定义在所有欧几里得空间上的函数的分解到其在拉普拉斯算子连续谱中的分量[[7]]<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hilbert_space#Fourier_analysis" title="Hilbert空间中的傅里叶分析">7</a>。</p>
<p>拉普拉斯矩阵是一个实对称矩阵，上一节中提到实对称矩阵的一些优良性质，存在标准正交基构成的矩阵$U$及其对应的特征值对角矩阵$\Lambda$使得拉普拉斯矩阵$L$满足$U^TLU=\Lambda$。其中$U=[\vec u_1, \vec u_2,…, \vec u_n]$中的$\vec u_i$可以视为离散的傅里叶基，而所对应的特征值则为频率。如果每个结点的特征（简单起见，每个结点只有一维特征）组成的向量$\vec x=[x_1,x_2,…,x_n]$，那么如果我们将其转化为频域向量$\vec y=U^T\vec x$，正如我们在第一节、第二节提到的方式一样。然后就可以在频域中操作，再转换为时域即可。</p>
<h2 id="图的频域分量"><a href="#图的频域分量" class="headerlink" title="图的频域分量"></a>图的频域分量</h2><p>在讲图（Graph）之前，我们可以先聊聊图像（Image）。图像通常由一个矩阵表示，矩阵中的每一个值为像素点的值。根据通道数，图像又可以分为彩色图像和灰度图像。</p>
<p>事实上图像它也可表示称图的结构。如下图所示[[11]]<a target="_blank" rel="noopener" href="https://distill.pub/2021/gnn-intro/" title="图像与图">11</a>：</p>
<p><img src="https://imagehost.vitaminz-image.top/gnn-note-8.png"></p>
<p>其中，每个像素与上下左右对角线相邻接。</p>
<p>为了方便可视化图数据的频域分量，我们使用上述图数据的结构，即讲图像转化为图。具体执行如下操作</p>
<figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">step1</span>. 选择(<span class="number">10</span>, <span class="number">10</span>)尺寸的图像结构，</span><br><span class="line"><span class="attribute">step2</span>. 根据每个像素与上下左右对角线相邻接的规则构造邻接矩阵A, shape: (<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="attribute">step3</span>. 根据邻接矩阵A构造拉普拉斯矩阵L, shape: (<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="attribute">step4</span>. 对拉普拉斯矩阵进行对角化, 求得特征基矩阵U, shape(<span class="number">100</span>, <span class="number">100</span>)，对角矩阵P, shape(<span class="number">100</span>, <span class="number">100</span>)，注意上述<span class="number">2</span>个矩阵均按特征值的大小升序排序。</span><br><span class="line"><span class="attribute">step5</span>. 取特征值大小前<span class="number">10</span>的特征基，每个特征基重构为图像结构Iamge, shape(<span class="number">10</span>, <span class="number">10</span>)，进行可视化</span><br></pre></td></tr></tbody></table></figure>

<p>经过以上的操作后，最后的结果如下图所示，特征值按从小到大排序。这就是每个频率所对应的分量。可以看到图像一开始是纯色的，后来开始出现了类似波形的渐变过程，随着频率的增加，其波形也越复杂。</p>
<p><img src="https://imagehost.vitaminz-image.top/gnn-note-9.png"></p>
<p>以下为实现代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getLaplacianOfImage</span>(<span class="params">M, N</span>):</span><br><span class="line">    idx = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M * N)]).reshape((M, N))</span><br><span class="line">    tmp = np.ones((M+<span class="number">2</span>, N+<span class="number">2</span>), dtype=np.int32) * M * N</span><br><span class="line">    tmp[<span class="number">1</span>:M+<span class="number">1</span>, <span class="number">1</span>:N+<span class="number">1</span>] = idx</span><br><span class="line">    <span class="comment"># print(tmp)</span></span><br><span class="line">    directH = [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">    directV = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    A = np.zeros((M * N + <span class="number">1</span>, M * N + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, M+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> h, v <span class="keyword">in</span> <span class="built_in">zip</span>(directH, directV):</span><br><span class="line">                A[tmp[i, j], tmp[i+h, j+v]] = <span class="number">1</span></span><br><span class="line">    A = A[<span class="number">0</span>:M*N, <span class="number">0</span>:M*N]</span><br><span class="line">    L = np.diag(np.<span class="built_in">sum</span>(A, axis=<span class="number">1</span>)) - A</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line">M, N = <span class="number">10</span>, <span class="number">10</span></span><br><span class="line">L = getLaplacianOfImage(M, N)</span><br><span class="line"><span class="comment"># print(L)</span></span><br><span class="line"><span class="comment"># print(np.prod(L == L.T))</span></span><br><span class="line"><span class="comment"># plt.imshow(L)</span></span><br><span class="line">eigenvalue, featurevector = np.linalg.eigh(L)</span><br><span class="line">eigenbase = featurevector.T</span><br><span class="line"></span><br><span class="line">min_val, max_val = np.<span class="built_in">min</span>(eigenbase), np.<span class="built_in">max</span>(eigenbase)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">    fv = eigenbase[i].reshape((M, N))</span><br><span class="line">    plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(fv, vmin=min_val, vmax=max_val)</span><br><span class="line">    plt.colorbar(fraction=<span class="number">0.045</span>)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>写该博客的起因是学习GNN时，不理解为什么使用$L$矩阵，而不是用邻接矩阵或者其他实对称矩阵，以及在文献中[[13]]<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.07308" title="Li X, Zhu R, Cheng Y, et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily[J]. arXiv preprint arXiv:2205.07308, 2022.">13</a>看到$L$矩阵的谱分解其实就是傅里叶变换在图领域的应用。为了深入了解这二者的关系，捡起了以前学过但又未深入理解的知识。完成这篇博客，还是很有收获的。</p>
<p><span id="ckwx"> </span></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2></div><div id="donate"><link rel="stylesheet" type="text/css" href="/css/donate.css?v=1.0.0"><script type="text/javascript" src="/js/donate.js?v=1.0.0" successtext="复制成功!"></script><a class="pos-f tr3" id="github" href="https://github.com/Kaiyuan/donate-page" target="_blank" title="Github"></a><div id="DonateText">Donate</div><ul class="list pos-f" id="donateBox"><li id="AliPay" qr="/img/AlipayQR.jpg"></li><li id="WeChat" qr="/img/WechatQR.png"></li></ul><div class="pos-f left-100" id="QRBox"><div id="MainBox"></div></div></div><div class="tags"><a href="/tags/谱图理论"><i class="fa fa-tag">谱图理论</i></a><a href="/tags/傅里叶变换"><i class="fa fa-tag">傅里叶变换</i></a><a href="/tags/线性代数"><i class="fa fa-tag">线性代数</i></a><a href="/tags/频域"><i class="fa fa-tag">频域</i></a></div><div class="post-nav"><a class="next" href="/2022/06/20/dong-shou-xie-yi-ge-bian-yi-qi/">动手写一个编译器</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/avatar.png"/></a><p>海明威说：“这个世界很美好，值得我们为之奋斗。”我只同意后半句</p><a class="info-icon" href="mailto:lzeng1507@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/vitaminzl" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Markdown/">Markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/">计算机基础</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Typora/" style="font-size: 15px;">Typora</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" style="font-size: 15px;">编译原理</a> <a href="/tags/%E8%B0%B1%E5%9B%BE%E7%90%86%E8%AE%BA/" style="font-size: 15px;">谱图理论</a> <a href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">傅里叶变换</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 15px;">线性代数</a> <a href="/tags/%E9%A2%91%E5%9F%9F/" style="font-size: 15px;">频域</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/tu-graph-shu-ju-de-pin-yu/">上线测试</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/06/20/dong-shou-xie-yi-ge-bian-yi-qi/">动手写一个编译器</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/03/01/shang-xian-ce-shi/">上线测试</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">闲渔.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>